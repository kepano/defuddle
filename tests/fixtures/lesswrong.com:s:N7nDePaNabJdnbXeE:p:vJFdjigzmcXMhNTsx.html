<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta http-equiv="delegate-ch" content="sec-ch-dpr https://res.cloudinary.com;"/><meta http-equiv="Accept-CH" content="DPR, Viewport-Width, Width"/></head><body class="whiteBackground"><div hidden=""><!--$?--><template id="B:0"></template><!--/$--></div><!--$?--><template id="B:2"></template><!--/$--><!--$?--><template id="B:1"></template><!--/$--><div style="opacity:0.001;position:absolute;top:0;left:0">x</div><meta name="sentry-trace" content="6b22d20ed92f5b097760ef8a14805d75-f2460ea5e6eebdd6"/><meta name="baggage" content="sentry-environment=production,sentry-release=5a750e322c9bac1bbe79d4ad08f0d9dfc093a33d,sentry-public_key=1ab1949fc8d04608b43132f37bb2a1b0,sentry-trace_id=6b22d20ed92f5b097760ef8a14805d75,sentry-org_id=195791,sentry-sample_rand=0.8742896192915552"/><div hidden id="S:1"><div class="Layout-topLevelContainer"><div class="Layout-pageContent"><span class="Header-headerHeight"><div id="wrapper" class="wrapper PageBackgroundWrapper-wrapper"><noscript class="noscript-warning"> This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. </noscript><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRC765W" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><!--$--><div class="Header-root"><div style="height:64px" class="Header-headroom headroom-wrapper"><div class="headroom headroom--unfixed"><header class="Header-appBar Header-blackBackgroundAppBar"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root Header-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><h2 class="Typography-root Typography-title Header-title"><div class="Header-hideSmDown"><div class="Header-titleSubtitleContainer"><div class="Header-titleFundraiserContainer"><a class="Header-titleLink" href="/">LESSWRONG</a></div><span></span></div></div><div class="Header-hideMdUp Header-titleFundraiserContainer"><a class="Header-titleLink" href="/">LW</a></div></h2><div class="Header-rightHeaderItems"><div class="SearchBar-root"><div class="SearchBar-rootChild"><div class="SearchBar-searchInputArea SearchBar-searchInputAreaSmall"><div><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SearchBar-searchIconButton SearchBar-searchIconButtonSmall" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root SearchBar-searchIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></div><div></div></div></div></div><div class="UsersAccountMenu-root"><button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat" type="button" data-testid="user-signup-button"><span class="MuiButton-label"><span class="UsersAccountMenu-userButton">Login</span></span><span class="MuiTouchRipple-root"></span></button></div></div></div></header></div></div></div><!--/$--><div class="LWBackgroundImage-root"></div><div class="Layout-searchResultsArea"></div><!--&--><!--$--><!--&--><!--&--><!--&--><!--&--><div data-response-metadata="eyJzdGF0dXMiOjIwMH0="></div><div class="RouteRootClient-main"><div class=""><div class="RouteRootClient-centralColumn"><!--$?--><template id="B:3"></template><div class="DelayedLoading-spinner"><div class="DelayedLoading-bounce1"></div><div class="DelayedLoading-bounce2"></div><div class="DelayedLoading-bounce3"></div></div><!--/$--></div></div></div><div></div><div class="Footer-root"></div><!--$?--><template id="B:4"></template><!--/$--><!--/&--><!--/&--><!--/&--><!--/&--><!--/$--><!--/&--></div></span></div></div><div></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:1","S:1")</script><div hidden id="S:2"></div><script>$RC("B:2","S:2")</script><title>Simulators — LessWrong</title><meta name="description" content="This post explores the concept of simulators in AI, particularly self-supervised models like GPT. Janus argues that GPT and similar models are best u…"/><meta name="citation_title" content="Simulators"/><meta name="citation_author" content="janus"/><meta property="og:title" content="Simulators — LessWrong"/><meta property="og:description" content="This post explores the concept of simulators in AI, particularly self-supervised models like GPT. Janus argues that GPT and similar models are best u…"/><meta property="og:url" content="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"/><meta property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/v1/mirroredImages/splashArtImagePromptA%20marionette%20theater%20with%20multiple%20puppeteers%20manipulating%20different%20characters/kehilmgftixxsfu9txcb"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Simulators — LessWrong"/><meta name="twitter:description" content="This post explores the concept of simulators in AI, particularly self-supervised models like GPT. Janus argues that GPT and similar models are best u…"/><meta name="twitter:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/v1/mirroredImages/splashArtImagePromptA%20marionette%20theater%20with%20multiple%20puppeteers%20manipulating%20different%20characters/kehilmgftixxsfu9txcb"/><div hidden id="S:4"></div><script>$RC("B:4","S:4")</script><div hidden id="S:0"></div><script>$RC("B:0","S:0")</script><div hidden id="S:3"><div data-response-metadata="eyJzdGF0dXMiOjIwMH0="></div><template id="P:5"></template></div><div hidden id="S:5"><div><div class="ReadingProgressBar-readingProgressBar"></div><div class="PostsPage-splashHeaderImage"><div class="BestOfLWPostsPageSplashImage-root"><div class="BestOfLWPostsPageSplashImage-backgroundImageWrapper" style="opacity:1"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/splashArtImagePromptA%20marionette%20theater%20with%20multiple%20puppeteers%20manipulating%20different%20characters/kehilmgftixxsfu9txcb" class="BestOfLWPostsPageSplashImage-backgroundImage" alt="Background Image"/><div class="BestOfLWPostsPageSplashImage-overlayY"></div><div class="BestOfLWPostsPageSplashImage-overlayX"></div><div class="BestOfLWPostsPageSplashImage-overlayDiag"></div></div></div></div><div class="MultiToCLayout-root"><div class="MultiToCLayout-tableOfContents" style="grid-template-areas:&quot;... toc0 gap1 content0 gap2 rhs0 ...&quot;
&quot;... toc0 gap1 content1 gap2 rhs1 ...&quot;
&quot;... toc2 gap1 content2 gap2 rhs2 ...&quot;
&quot;... toc2 gap1 content3 gap2 rhs3 ...&quot;;grid-template-rows:min-content min-content min-content 1fr"><div style="grid-area:toc0" class="MultiToCLayout-toc MultiToCLayout-splashPageHeaderToc"><div class="MultiToCLayout-stickyBlockScroller MultiToCLayoutStickyBlockScroller"><div class="MultiToCLayout-stickyBlock"><div></div></div></div></div><div class="MultiToCLayout-gap1"></div><div class="MultiToCLayout-content" style="grid-area:content0"><div id="postBody" class="PostsPage-centralColumn PostsPage-postBody PostsPage-audioPlayerHidden"><div class="PostsPage-title"><div class="PostsPage-centralColumn"><div class="LWPostsPageHeader-root LWPostsPageHeader-rootWithSplashPageHeader"><div class="LWPostsPageHeader-sequenceNav"><div class="PostsTopSequencesNav-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SequencesNavigationLink-root SequencesNavigationLink-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M15.41 7.41L14 6l-6 6 6 6 1.41-1.41L10.83 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><span class=""><div class="PostsTopSequencesNav-title PostsTopSequencesNav-blackText"><a href="/s/N7nDePaNabJdnbXeE">Simulators</a></div></span><span><a href="/s/N7nDePaNabJdnbXeE/p/DSEwkvj8W7y8C3jau"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SequencesNavigationLink-root SequencesNavigationLink-normal" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></a></span></div></div><div><span class="LWPostsPageHeader-topRight"><div class="LWPostsPageHeaderTopRight-root"><div class="LWPostsPageHeaderTopRight-tagList"><span class="FooterTagList-root FooterTagList-alignRight"><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/simulator-theory"><span class="FooterTag-name">Simulator Theory</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/language-models-llms"><span class="FooterTag-name">Language Models (LLMs)</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/llm-personas"><span class="FooterTag-name">LLM Personas</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/gpt"><span class="FooterTag-name">GPT</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/outer-alignment"><span class="FooterTag-name">Outer Alignment</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/simulation-1"><span class="FooterTag-name">Simulation</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/corrigibility-1"><span class="FooterTag-name">Corrigibility</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/deconfusion"><span class="FooterTag-name">Deconfusion</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/myopia"><span class="FooterTag-name">Myopia</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/oracle-ai"><span class="FooterTag-name">Oracle AI</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/tool-ai"><span class="FooterTag-name">Tool AI</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-noBackground"><a href="/w/ai"><span class="FooterTag-name">AI</span></a></span></span><a class="FooterTagList-postTypeLink" href="/recommendations"><span class="LWTooltip-root"><div class="FooterTagList-frontpageOrPersonal FooterTagList-noBackground FooterTagList-neverCoreStyling">Curated</div></span></a></span></div><div class="LWPostsPageHeaderTopRight-audioToggle"><span class="LWTooltip-root AudioToggle-togglePodcastContainer"><a href="#"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true" class="AudioToggle-audioIcon ForumIcon-root"><g stroke-width="1.5" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" d="M19.114 5.636a9 9 0 010 12.728M16.463 8.288a5.25 5.25 0 010 7.424M6.75 8.25l4.72-4.72a.75.75 0 011.28.53v15.88a.75.75 0 01-1.28.53l-4.72-4.72H4.51c-.88 0-1.704-.507-1.938-1.354A9.01 9.01 0 012.25 12c0-.83.112-1.633.322-2.396C2.806 8.756 3.63 8.25 4.51 8.25H6.75z"></path></g></svg></a></span></div><div class="LWPostsPageHeaderTopRight-vote"><div class="LWPostsPageTopHeaderVote-voteBlockHorizontal"><span><div class="LWPostsPageTopHeaderVote-upvoteHorizontal"><button class="VoteArrowIconSolid-root VoteButton-root VoteArrowIconSolid-up" type="button"><span class="VoteButton-inner"><svg width="10" height="10" viewBox="0 0 9 6" fill="currentColor" class="VoteArrowIconSolid-smallArrow"><path d="M4.11 0.968C4.31 0.725 4.69 0.725 4.89 0.968L8.16 4.932C8.42 5.26 8.19 5.75 7.77 5.75H1.23C0.808 5.75 0.576 5.26 0.845 4.93L4.11 0.968Z"></path></svg></span></button></div></span><div class="LWPostsPageTopHeaderVote-voteScoresHorizontal"><span><div><h1 class="Typography-root Typography-headline LWPostsPageTopHeaderVote-voteScore">690</h1></div></span></div><span><div class="LWPostsPageTopHeaderVote-downvoteHorizontal"><button class="VoteArrowIconSolid-root VoteButton-root VoteArrowIconSolid-down" type="button"><span class="VoteButton-inner"><svg width="10" height="10" viewBox="0 0 9 6" fill="currentColor" class="VoteArrowIconSolid-smallArrow"><path d="M4.11 0.968C4.31 0.725 4.69 0.725 4.89 0.968L8.16 4.932C8.42 5.26 8.19 5.75 7.77 5.75H1.23C0.808 5.75 0.576 5.26 0.845 4.93L4.11 0.968Z"></path></svg></span></button></div></span></div></div><div class="PostActionsButton-root LWPostsPageHeaderTopRight-postActionsButton"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div></span><span class="LWPostsPageHeader-audioPlayerWrapper"><div class="PostsAudioPlayerWrapper-embeddedPlayer PostsAudioPlayerWrapper-hideEmbeddedPlayer"><div id="buzzsprout-player-11261126" class="PostsPodcastPlayer-embeddedPlayer"></div><ul class="PostsPodcastPlayer-podcastIconList"><li class="PostsPodcastPlayer-podcastIcon"><a href="https://podcasts.apple.com/us/podcast/lesswrong-curated-podcast/id1630783021"><svg xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" width="164.8566px" height="40px" viewBox="0 0 164.8566 40" enable-background="new 0 0 164.8566 40" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"><g id="US"><path fill="#FFFFFF" d="M155.3273,0H9.5347C9.168,0,8.8057,0,8.4399,0.002C8.1338,0.0039,7.8301,0.0098,7.521,0.0146   C6.8555,0.0312,6.1816,0.0723,5.5171,0.1914c-0.6694,0.1172-1.2915,0.3174-1.9009,0.627C3.0176,1.125,2.4707,1.5234,1.9976,1.9971   C1.519,2.4707,1.1226,3.0195,0.8193,3.6182c-0.311,0.6084-0.5083,1.2334-0.625,1.9033c-0.1206,0.6621-0.1621,1.332-0.1792,2.002   C0.0059,7.8301,0.0049,8.1377,0,8.4443c0,0.3623,0,0.7256,0,1.0918v20.9287c0,0.3692,0,0.7305,0,1.0938   c0.0049,0.3105,0.0059,0.6113,0.0151,0.9219c0.0171,0.6699,0.0586,1.3398,0.1792,2.0019c0.1167,0.6699,0.314,1.2979,0.625,1.9043   c0.3032,0.5957,0.6997,1.1446,1.1782,1.6143c0.4731,0.4775,1.02,0.875,1.6187,1.1787c0.6094,0.3125,1.2314,0.5098,1.9009,0.6308   c0.6645,0.1192,1.3384,0.1583,2.0039,0.1768c0.3091,0.0068,0.6128,0.0107,0.9189,0.0107C8.8057,40,9.168,40,9.5347,40h145.7927   c0.3594,0,0.7246,0,1.084-0.002c0.3047,0,0.6172-0.0039,0.9219-0.0107c0.6699-0.0185,1.3418-0.0576,2-0.1768   c0.6699-0.121,1.2929-0.3183,1.9082-0.6308c0.5976-0.3037,1.1445-0.7012,1.6172-1.1787c0.4765-0.4697,0.873-1.0186,1.1816-1.6143   c0.3066-0.6064,0.5059-1.2344,0.6191-1.9043c0.1231-0.6621,0.1621-1.332,0.1856-2.0019c0.0039-0.3106,0.0039-0.6114,0.0039-0.9219   c0.0078-0.3633,0.0078-0.7246,0.0078-1.0938V9.5361c0-0.3662,0-0.7295-0.0078-1.0918c0-0.3066,0-0.6143-0.0039-0.9209   c-0.0235-0.6699-0.0625-1.3398-0.1856-2.002c-0.1132-0.6699-0.3125-1.2949-0.6191-1.9033   c-0.3086-0.5986-0.7051-1.1475-1.1816-1.6211c-0.4727-0.4736-1.0196-0.8721-1.6172-1.1787   c-0.6153-0.3096-1.2383-0.5098-1.9082-0.627c-0.6582-0.1191-1.3301-0.1602-2-0.1768c-0.3047-0.0049-0.6172-0.0107-0.9219-0.0127   C156.0519,0,155.6867,0,155.3273,0L155.3273,0z"></path><path d="M 8.4448 39.125 C 8.1401 39.125 7.8428 39.1211 7.540500000000001 39.1143 C 6.981400000000001 39.0986 6.318300000000001 39.0674 5.6714 38.9512 C 5.061 38.8408 4.5186 38.6611 4.0147 38.4033 C 3.4932000000000003 38.1387 3.0245000000000006 37.7969 2.6177 37.386700000000005 C 2.2036000000000002 36.980500000000006 1.8633000000000002 36.51370000000001 1.5972000000000002 35.9902 C 1.3379 35.4854 1.1607 34.943400000000004 1.0542000000000002 34.333 C 0.9321000000000003 33.660199999999996 0.9009000000000003 32.9775 0.8877000000000003 32.458 C 0.8814000000000003 32.247099999999996 0.8731000000000003 31.5449 0.8731000000000003 31.5449 L 0.8731000000000003 8.4443 C 0.8731000000000003 8.4443 0.8819000000000004 7.7529 0.8877000000000003 7.5498 C 0.9009000000000003 7.0259 0.9321000000000003 6.3438 1.0532000000000004 5.6777 C 1.1602000000000001 5.064 1.3374 4.5225 1.5967 4.0156 C 1.8623 3.4912 2.2031 3.022 2.6118 2.6177 C 3.0254000000000003 2.2036000000000002 3.4961 1.8609 4.0141 1.5952000000000002 C 4.5278 1.3345000000000002 5.0693 1.1562000000000001 5.6674 1.0513000000000001 C 6.3407 0.9307000000000001 7.0234 0.9004000000000001 7.5428999999999995 0.8872000000000001 L 8.4453 0.8750000000000001 L 156.4064 0.8750000000000001 L 157.3195 0.8877000000000002 C 157.8322 0.9004000000000002 158.5148 0.9307000000000002 159.1779 1.0503000000000002 C 159.7804 1.1558000000000002 160.3254 1.3350000000000002 160.84879999999998 1.5982000000000003 C 161.36149999999998 1.8609000000000002 161.8312 2.2037000000000004 162.2404 2.6138000000000003 C 162.6496 3.0200000000000005 162.9923 3.4917000000000007 163.2638 4.0181000000000004 C 163.5216 4.5293 163.69740000000002 5.0708 163.799 5.667000000000001 C 163.9152 6.2979 163.9513 6.9458 163.9728 7.554200000000001 C 163.97570000000002 7.837400000000001 163.97570000000002 8.1416 163.97570000000002 8.4443 C 163.98360000000002 8.8193 163.98360000000002 9.1762 163.98360000000002 9.536100000000001 L 163.98360000000002 30.4648 C 163.98360000000002 30.8281 163.98360000000002 31.1826 163.97570000000002 31.54 C 163.97570000000002 31.865199999999998 163.97570000000002 32.1631 163.97180000000003 32.469699999999996 C 163.95130000000003 33.0586 163.91520000000003 33.7061 163.80090000000004 34.32319999999999 C 163.69740000000004 34.936499999999995 163.52160000000003 35.47849999999999 163.26090000000005 35.993199999999995 C 162.99140000000006 36.512699999999995 162.64860000000004 36.98239999999999 162.24530000000004 37.378899999999994 C 161.83220000000006 37.796899999999994 161.36340000000004 38.137699999999995 160.84590000000003 38.401399999999995 C 160.32730000000004 38.66499999999999 159.79800000000003 38.8398 159.17790000000002 38.95119999999999 C 158.53730000000002 39.06739999999999 157.87420000000003 39.09859999999999 157.30870000000002 39.11429999999999 C 157.0158 39.12109999999999 156.7091 39.12499999999999 156.4113 39.12499999999999 L 155.3273 39.126999999999995 L 8.4448 39.125" style="fill:rgb(255, 255, 255)"></path><g id="Rectangle-Solid" fill="#FFFFFF" stroke="#9AA0A6" stroke-width="0.56"><rect id="Rectangle" x="0.28" y="0.28" width="163.8566px" height="39px" rx="8.96"></rect></g><g id="XMLID_632_"><linearGradient id="XMLID_2_" gradientUnits="userSpaceOnUse" x1="20.0983" y1="7.4183" x2="20.0983" y2="32.5813"><stop offset="0" style="stop-color:#000000"></stop><stop offset="1" style="stop-color:#000000"></stop></linearGradient><path id="XMLID_662_" fill-rule="evenodd" clip-rule="evenodd" fill="url(#XMLID_2_)" d="M32.1821,29.2281    c-0.2446,0.6213-0.809,1.4961-1.6094,2.1123c-0.4621,0.3559-1.0138,0.6961-1.7716,0.9206    c-0.8078,0.2394-1.8028,0.3203-3.0412,0.3203H14.4366c-1.2383,0-2.2333-0.0809-3.0412-0.3203    c-0.7578-0.2245-1.3094-0.5647-1.7716-0.9206c-0.8003-0.6163-1.3648-1.491-1.6094-2.1123    c-0.4927-1.2518-0.4977-2.6721-0.4977-3.5666l0,0V14.3381l0,0c0-0.8945,0.005-2.3148,0.4977-3.5666    c0.2446-0.6213,0.809-1.4961,1.6094-2.1123c0.4621-0.3559,1.0138-0.6961,1.7716-0.9206c0.8078-0.2394,1.8028-0.3203,3.0412-0.3203    l0,0H25.76l0,0c1.2383,0,2.2333,0.0809,3.0412,0.3203c0.7578,0.2245,1.3094,0.5648,1.7716,0.9206    c0.8003,0.6163,1.3648,1.491,1.6094,2.1123c0.4927,1.2518,0.4977,2.6721,0.4977,3.5666v11.3234    C32.6798,26.5559,32.6748,27.9762,32.1821,29.2281z"></path><path id="XMLID_660_" fill-rule="evenodd" clip-rule="evenodd" fill="none" d="M32.1821,29.2281    c-0.2446,0.6213-0.809,1.4961-1.6094,2.1123c-0.4621,0.3559-1.0138,0.6961-1.7716,0.9206    c-0.8078,0.2394-1.8028,0.3203-3.0412,0.3203H14.4366c-1.2383,0-2.2333-0.0809-3.0412-0.3203    c-0.7578-0.2245-1.3094-0.5647-1.7716-0.9206c-0.8003-0.6163-1.3648-1.491-1.6094-2.1123    c-0.4927-1.2518-0.4977-2.6721-0.4977-3.5666l0,0V14.3381l0,0c0-0.8945,0.005-2.3148,0.4977-3.5666    c0.2446-0.6213,0.809-1.4961,1.6094-2.1123c0.4621-0.3559,1.0138-0.6961,1.7716-0.9206c0.8078-0.2394,1.8028-0.3203,3.0412-0.3203    l0,0H25.76l0,0c1.2383,0,2.2333,0.0809,3.0412,0.3203c0.7578,0.2245,1.3094,0.5648,1.7716,0.9206    c0.8003,0.6163,1.3648,1.491,1.6094,2.1123c0.4927,1.2518,0.4977,2.6721,0.4977,3.5666v11.3234    C32.6798,26.5559,32.6748,27.9762,32.1821,29.2281z"></path><g id="XMLID_648_"><g><path fill="#FFFFFF" d="M21.9232,21.8815c-0.3776-0.3984-1.0408-0.6538-1.8237-0.6538c-0.783,0-1.4462,0.2554-1.8238,0.6538      c-0.197,0.2079-0.3012,0.421-0.334,0.7246c-0.0636,0.5879-0.0279,1.0941,0.041,1.9034c0.0656,0.7712,0.1902,1.7996,0.3528,2.847      c0.1157,0.7454,0.2097,1.1477,0.2953,1.4358c0.1388,0.4669,0.6575,0.8752,1.4687,0.8752c0.8111,0,1.3298-0.4084,1.4686-0.8752      c0.0856-0.2881,0.1796-0.6904,0.2953-1.4358c0.1626-1.0474,0.2872-2.0758,0.3528-2.847      c0.0689-0.8094,0.1046-1.3156,0.041-1.9034C22.2244,22.3026,22.1202,22.0894,21.9232,21.8815z M18.0421,18.3342      c0,1.1373,0.922,2.0593,2.0593,2.0593c1.1374,0,2.0594-0.922,2.0594-2.0593c0-1.1374-0.922-2.0594-2.0594-2.0594      C18.9641,16.2748,18.0421,17.1968,18.0421,18.3342z M20.0759,10.2072c-4.8163,0.0137-8.7628,3.921-8.8224,8.737      c-0.0484,3.9013,2.442,7.2385,5.9195,8.4617c0.0844,0.0297,0.1701-0.0404,0.1568-0.1289      c-0.0456-0.3029-0.0886-0.6073-0.1281-0.9066c-0.014-0.1058-0.0814-0.197-0.1791-0.2397      c-2.7486-1.2008-4.6679-3.9574-4.6371-7.1505c0.0404-4.1904,3.4622-7.6066,7.6526-7.6405c4.2839-0.0346,7.78,3.4402,7.78,7.7162      c0,3.1611-1.9109,5.8839-4.6381,7.0751c-0.0977,0.0427-0.1647,0.1342-0.1786,0.2399c-0.0395,0.2992-0.0825,0.6035-0.128,0.9061      c-0.0134,0.0885,0.0723,0.1586,0.1567,0.129c3.4443-1.2115,5.9202-4.4968,5.9202-8.35      C28.9501,14.1681,24.9668,10.1932,20.0759,10.2072z M19.916,14.2667c2.7303-0.1057,4.9851,2.0847,4.9851,4.7919      c0,1.3768-0.5833,2.6198-1.5156,3.4952c-0.0791,0.0742-0.1216,0.1796-0.1155,0.2879c0.0186,0.3293,0.0118,0.6493-0.0101,1.0107      c-0.0059,0.0963,0.1021,0.1578,0.1818,0.1035c1.5634-1.0684,2.5917-2.8649,2.5917-4.8973c0-3.3464-2.7874-6.0541-6.1623-5.9233      c-3.1364,0.1216-5.6527,2.7075-5.6929,5.8461c-0.0264,2.0642,1.0082,3.8929,2.5917,4.9747      c0.0795,0.0543,0.1872-0.0073,0.1813-0.1035c-0.022-0.3616-0.0287-0.6816-0.0102-1.0109      c0.0061-0.1082-0.0363-0.2136-0.1152-0.2877c-0.9614-0.9026-1.5518-2.1961-1.5142-3.6241      C15.3778,16.4169,17.4039,14.364,19.916,14.2667z"></path></g></g></g><g><path d="M46.223,27.8223h-4.7334l-1.1367,3.3564H38.348l4.4834-12.4189h2.083l4.4834,12.4189h-2.0391    L46.223,27.8223z M41.9798,26.2734h3.752l-1.8496-5.4482h-0.0518L41.9798,26.2734z"></path><path d="M59.0804,26.6523c0,2.8135-1.5059,4.6211-3.7783,4.6211c-1.29,0-2.3145-0.5771-2.8486-1.584h-0.043v4.4844    h-1.8584V22.125h1.7988v1.5059h0.0342c0.5166-0.9717,1.6182-1.6006,2.8828-1.6006    C57.5657,22.0303,59.0804,23.8467,59.0804,26.6523z M57.1702,26.6523c0-1.833-0.9473-3.0381-2.3926-3.0381    c-1.4199,0-2.375,1.2305-2.375,3.0381c0,1.8242,0.9551,3.0459,2.375,3.0459C56.223,29.6982,57.1702,28.502,57.1702,26.6523z"></path><path d="M69.0452,26.6523c0,2.8135-1.5059,4.6211-3.7783,4.6211c-1.29,0-2.3144-0.5771-2.8486-1.584h-0.043v4.4844    h-1.8584V22.125h1.7988v1.5059h0.0342c0.5166-0.9717,1.6182-1.6006,2.8828-1.6006    C67.5306,22.0303,69.0452,23.8467,69.0452,26.6523z M67.1351,26.6523c0-1.833-0.9473-3.0381-2.3926-3.0381    c-1.4199,0-2.375,1.2305-2.375,3.0381c0,1.8242,0.9551,3.0459,2.375,3.0459C66.1878,29.6982,67.1351,28.502,67.1351,26.6523z"></path><path d="M70.5501,18.7598h1.8594v12.4189h-1.8594V18.7598z"></path><path d="M82.0853,28.5195c-0.25,1.6436-1.8506,2.7715-3.8984,2.7715c-2.6338,0-4.2686-1.7646-4.2686-4.5957    c0-2.8398,1.6436-4.6816,4.1904-4.6816c2.5049,0,4.0801,1.7207,4.0801,4.4658v0.6367h-6.3945v0.1123    c0,1.5488,0.9727,2.5645,2.4355,2.5645c1.0322,0,1.8418-0.4902,2.0908-1.2734H82.0853z M75.803,25.8174h4.5264    c-0.043-1.3857-0.9297-2.2979-2.2207-2.2979C76.8265,23.5195,75.8978,24.4492,75.803,25.8174z"></path><path d="M92.0872,18.7598c2.4092,0,4.0879,1.6611,4.0879,4.0801c0,2.4268-1.7129,4.0957-4.1484,4.0957h-2.668    v4.2432H87.431V18.7598H92.0872z M89.3587,25.3184h2.2119c1.6777,0,2.6328-0.9043,2.6328-2.4697    c0-1.5664-0.9551-2.4624-2.624-2.4624h-2.2207V25.3184z"></path><path d="M96.8538,26.6523c0-2.8486,1.6777-4.6387,4.2939-4.6387c2.625,0,4.2949,1.79,4.2949,4.6387    c0,2.8564-1.6611,4.6387-4.2949,4.6387C98.5149,31.291,96.8538,29.5088,96.8538,26.6523z M103.5491,26.6523    c0-1.9541-0.8955-3.1074-2.4014-3.1074s-2.4014,1.1621-2.4014,3.1074c0,1.9619,0.8955,3.1064,2.4014,3.1064    S103.5491,28.6143,103.5491,26.6523z"></path><path d="M106.7474,26.6523c0-2.7969,1.54-4.6221,3.7861-4.6221c1.2998,0,2.3242,0.6025,2.8398,1.6006h0.0352    v-4.8711h1.8672v12.4189h-1.8076v-1.54h-0.0342c-0.5332,1.0322-1.5664,1.6348-2.8828,1.6348    C108.2874,31.2734,106.7474,29.4492,106.7474,26.6523z M108.6487,26.6523c0,1.8584,0.9473,3.0459,2.3926,3.0459    c1.4287,0,2.3838-1.2051,2.3838-3.0459c0-1.8252-0.9551-3.0381-2.3838-3.0381C109.596,23.6143,108.6487,24.8105,108.6487,26.6523z    "></path><path d="M123.1839,25.2402c-0.1631-0.9551-0.9121-1.6689-2.1338-1.6689c-1.4287,0-2.376,1.1963-2.376,3.0811    c0,1.9277,0.9561,3.0889,2.3926,3.0889c1.1533,0,1.9111-0.5762,2.1172-1.626h1.79c-0.2061,1.9014-1.7295,3.1758-3.9238,3.1758    c-2.582,0-4.2686-1.7646-4.2686-4.6387c0-2.8145,1.6865-4.6387,4.251-4.6387c2.3232,0,3.7695,1.4629,3.9248,3.2266H123.1839z"></path><path d="M126.1126,28.6055c0-1.583,1.2129-2.5391,3.3643-2.668l2.4785-0.1377v-0.6885    c0-1.0068-0.6621-1.5752-1.7891-1.5752c-1.0332,0-1.7559,0.4912-1.9023,1.2744h-1.7383c0.0518-1.6357,1.5742-2.7969,3.6914-2.7969    c2.1602,0,3.5889,1.1787,3.5889,2.96v6.2051h-1.7812v-1.4893h-0.043c-0.5254,1.0068-1.6699,1.6445-2.8574,1.6445    C127.3519,31.334,126.1126,30.2324,126.1126,28.6055z M131.9554,27.7881v-0.6973l-2.2285,0.1377    c-1.1104,0.0693-1.7383,0.5508-1.7383,1.3252c0,0.792,0.6543,1.3086,1.6524,1.3086    C130.9407,29.8623,131.9554,28.9668,131.9554,27.7881z"></path><path d="M138.9935,22.0225c2.0059,0,3.4424,1.1094,3.4854,2.71h-1.7471c-0.0771-0.7998-0.7568-1.29-1.79-1.29    c-1.0068,0-1.6777,0.4639-1.6777,1.1699c0,0.542,0.4473,0.9033,1.3857,1.1357l1.5234,0.3535    c1.8242,0.4385,2.5127,1.1094,2.5127,2.4355c0,1.6348-1.5488,2.7539-3.7607,2.7539c-2.1348,0-3.5713-1.0938-3.709-2.7461h1.8408    c0.1299,0.8691,0.8262,1.334,1.9541,1.334c1.1104,0,1.8076-0.4561,1.8076-1.1787c0-0.5596-0.3447-0.8604-1.291-1.1016    l-1.6182-0.3955c-1.6357-0.3965-2.4619-1.2314-2.4619-2.4873C135.4476,23.1152,136.8851,22.0225,138.9935,22.0225z"></path><path d="M146.7239,19.9819v2.1431h1.7217v1.4717h-1.7217v4.9912c0,0.7754,0.3447,1.1367,1.1016,1.1367    c0.1895,0,0.4912-0.0264,0.6113-0.043v1.4629c-0.2061,0.0518-0.6191,0.0859-1.0322,0.0859c-1.833,0-2.5479-0.6885-2.5479-2.4443    v-5.1895h-1.3164V22.125h1.3164v-2.1431H146.7239z"></path><path d="M153.4427,22.0225c2.0059,0,3.4424,1.1094,3.4854,2.71h-1.7471c-0.0771-0.7998-0.7568-1.29-1.79-1.29    c-1.0068,0-1.6777,0.4639-1.6777,1.1699c0,0.542,0.4473,0.9033,1.3857,1.1357l1.5234,0.3535    c1.8242,0.4385,2.5127,1.1094,2.5127,2.4355c0,1.6348-1.5488,2.7539-3.7607,2.7539c-2.1348,0-3.5713-1.0938-3.709-2.7461h1.8408    c0.1299,0.8691,0.8262,1.334,1.9541,1.334c1.1104,0,1.8076-0.4561,1.8076-1.1787c0-0.5596-0.3447-0.8604-1.291-1.1016    l-1.6182-0.3955c-1.6357-0.3965-2.4619-1.2314-2.4619-2.4873C149.8968,23.1152,151.3343,22.0225,153.4427,22.0225z"></path></g><g><path d="M42.9578,14.6611h-3.7217V8.6943h0.9263v5.1147h2.7954V14.6611z"></path><path d="M44.0345,8.8267c0-0.3105,0.2441-0.5459,0.5747-0.5459c0.3311,0,0.5747,0.2354,0.5747,0.5459    c0,0.3057-0.2437,0.5415-0.5747,0.5415C44.2786,9.3682,44.0345,9.1323,44.0345,8.8267z M44.1668,10.1582h0.8848v4.5029h-0.8848    V10.1582z"></path><path d="M48.1307,10.0713c1.0132,0,1.6748,0.4712,1.7612,1.2651h-0.8516    c-0.083-0.3306-0.4053-0.5415-0.9097-0.5415c-0.4966,0-0.8726,0.2354-0.8726,0.5869c0,0.269,0.2275,0.4385,0.7153,0.5503    l0.7485,0.1733c0.856,0.1987,1.2568,0.5669,1.2568,1.2285c0,0.8477-0.7896,1.4141-1.8647,1.4141    c-1.0713,0-1.77-0.4839-1.8486-1.2817h0.8892c0.1118,0.3472,0.4424,0.562,0.98,0.562c0.5542,0,0.9473-0.248,0.9473-0.6079    c0-0.2686-0.2109-0.4424-0.6621-0.5498l-0.7856-0.1821c-0.856-0.2026-1.2529-0.5869-1.2529-1.2568    C46.3812,10.6333,47.1131,10.0713,48.1307,10.0713z"></path><path d="M52.346,9.0371v1.1416h0.9756v0.7485H52.346v2.3154c0,0.4717,0.1943,0.6782,0.6367,0.6782    c0.1362,0,0.2148-0.0083,0.3389-0.0205v0.7402c-0.1445,0.0244-0.3101,0.0454-0.4839,0.0454c-0.9883,0-1.3809-0.3477-1.3809-1.2158    v-2.543h-0.7153v-0.7485h0.7153V9.0371H52.346z"></path><path d="M58.2532,13.4453c-0.2026,0.8066-0.9224,1.3027-1.9517,1.3027c-1.2905,0-2.0801-0.8848-2.0801-2.3242    c0-1.439,0.8062-2.3525,2.0757-2.3525c1.2529,0,2.0098,0.856,2.0098,2.27v0.3101h-3.1802v0.0498    c0.0293,0.7896,0.4883,1.29,1.1992,1.29c0.5376,0,0.9058-0.1943,1.0713-0.5459H58.2532z M55.1268,11.9941h2.2744    c-0.0205-0.707-0.4507-1.1665-1.1079-1.1665C55.6356,10.8276,55.1766,11.291,55.1268,11.9941z"></path><path d="M59.4993,10.1582h0.856v0.7153h0.0664c0.2188-0.5005,0.6655-0.8022,1.3438-0.8022    c1.0049,0,1.5591,0.6035,1.5591,1.6748v2.915h-0.8892v-2.6919c0-0.7236-0.3145-1.0835-0.9717-1.0835    c-0.6577,0-1.0752,0.4385-1.0752,1.1411v2.6343h-0.8892V10.1582z"></path><path d="M66.9183,12.4077c0-1.4517,0.8105-2.3364,2.1255-2.3364c1.3105,0,2.1211,0.8848,2.1211,2.3364    c0,1.4595-0.8062,2.3403-2.1211,2.3403C67.7244,14.748,66.9183,13.8672,66.9183,12.4077z M70.2513,12.4077    c0-0.9761-0.4385-1.5469-1.2075-1.5469c-0.7734,0-1.2075,0.5708-1.2075,1.5469c0,0.9839,0.4341,1.5503,1.2075,1.5503    C69.8128,13.958,70.2513,13.3877,70.2513,12.4077z"></path><path d="M72.3572,10.1582h0.856v0.7153h0.0664c0.2192-0.5005,0.6655-0.8022,1.3438-0.8022    c1.0049,0,1.5591,0.6035,1.5591,1.6748v2.915h-0.8892v-2.6919c0-0.7236-0.3145-1.0835-0.9717-1.0835    c-0.6577,0-1.0752,0.4385-1.0752,1.1411v2.6343h-0.8892V10.1582z"></path></g></g><g></g></svg></a></li><li class="PostsPodcastPlayer-podcastIcon"><a href="https://open.spotify.com/show/7vqBzO0ejqiLiXyTECEeBY"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="165px" height="40px" viewBox="0 0 165 40" version="1.1"><title>spotify-podcast-badge-wht-blk-165x40</title><desc>Created with Sketch.</desc><g id="spotify-podcast-badge-wht-blk-165x40" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Rectangle-Solid" fill="#FFFFFF" stroke="#9AA0A6" stroke-width="0.56"><rect id="Rectangle" x="0.28" y="0.28" width="164px" height="39px" rx="8.96"></rect></g><g id="Group-2-Copy-7"><path d="M155.462317,0.5 L9.54298794,0.5 C9.23718907,0.500000061 9.10222971,0.500057382 8.9333245,0.500344823 C8.75914935,0.500641232 8.60147796,0.501164881 8.4503371,0.501990385 C8.31093757,0.503042293 8.31093757,0.503042293 8.17123664,0.504701114 C8.05145483,0.50622126 7.95430604,0.507728185 7.73523354,0.511324656 C7.63560663,0.512955798 7.63560663,0.512955798 7.53999468,0.514444796 C6.77532204,0.533501958 6.17590622,0.582235722 5.60805245,0.683921078 C4.97373089,0.794883016 4.40022513,0.982751407 3.84712442,1.26350199 C3.29467346,1.54621893 2.78971907,1.91364998 2.350874,2.35265774 C1.9136357,2.78495239 1.54752652,3.28900419 1.26513746,3.84593553 C0.985573621,4.39236294 0.798895233,4.96583863 0.686361558,5.611175 C0.583694388,6.17433335 0.534557489,6.76869157 0.514887789,7.53850954 C0.510904757,7.67113311 0.508296788,7.80771984 0.505899647,7.99045837 C0.50486648,8.07179097 0.50486648,8.07179097 0.503895865,8.15352806 C0.502129327,8.30229721 0.502129327,8.30229721 0.5,8.4443 C0.5,8.98825 0.5,8.98825 0.5,9.5361 L0.5,30.4648 C0.5,30.904206 0.5,30.904206 0.5,31.3385814 C0.5,31.4485499 0.5,31.4485499 0.499937643,31.5507036 C0.502129454,31.7019465 0.502129454,31.7019465 0.503895689,31.8505134 C0.504852178,31.9308028 0.504852178,31.9308028 0.505868535,32.0105901 C0.508268666,32.1932042 0.510882868,32.330397 0.514950025,32.46773 C0.53455833,33.2352268 0.583696518,33.8295783 0.687037849,34.3965174 C0.799275611,35.040243 0.986350407,35.6158457 1.26553381,36.1597408 C1.54826938,36.7147517 1.914868,37.2179189 2.35426946,37.6489326 C2.78903353,38.087359 3.2925666,38.4535299 3.84733733,38.734707 C4.39339648,39.0144827 4.96441014,39.2017457 5.61010376,39.3183422 C6.16895541,39.4185036 6.75381439,39.4656174 7.53852502,39.4874207 C7.85797197,39.4944423 8.15615495,39.498 8.44996762,39.4980075 C8.60147796,39.4988351 8.75914935,39.4993588 8.9333245,39.4996552 C9.10222971,39.4999426 9.23718907,39.4999999 9.54298794,39.5 L155.462417,39.5 C155.732811,39.5 155.853496,39.4999583 156.004198,39.4997505 C156.203248,39.499476 156.378467,39.4989313 156.547359,39.498 C156.839864,39.498 157.147833,39.4943606 157.45627,39.4874902 C158.246626,39.4656826 158.829251,39.4186451 159.383,39.3184485 C160.029195,39.2018314 160.602236,39.01452 161.155288,38.7338785 C161.707735,38.4533688 162.211183,38.0870066 162.649415,37.6447625 C163.081256,37.2194537 163.449134,36.7156181 163.736761,36.1609378 C164.015456,35.6102074 164.202464,35.0320689 164.310967,34.3909277 C164.412834,33.8435049 164.459672,33.2868196 164.488329,32.4742169 C164.490132,32.3308112 164.491187,32.1827793 164.491715,32.0155813 C164.492114,31.8894402 164.492193,31.7888194 164.492309,31.5478582 C164.494473,31.439896 164.494473,31.439896 164.49608,31.3317328 C164.499348,31.0931827 164.5,30.9019181 164.5,30.4648 L164.5,9.5361 C164.5,8.99184779 164.498715,8.7523562 164.492193,8.4443 C164.492193,8.2144775 164.492112,8.1126965 164.491708,7.98543854 C164.491176,7.81794208 164.49012,7.67034488 164.488598,7.54094437 C164.459672,6.71708606 164.412834,6.16037793 164.30953,5.60477958 C164.202818,4.97382724 164.016273,4.39803029 163.738554,3.8473707 C163.450094,3.28832338 163.082566,2.78338505 162.646527,2.35037068 C162.210469,1.91386133 161.705543,1.54618981 161.157075,1.26502432 C160.595147,0.98252524 160.019849,0.794680145 159.382845,0.683323671 C158.822393,0.581999014 158.225384,0.533353753 157.462028,0.514435472 C157.37409,0.513008314 157.37409,0.513008314 157.285869,0.511551041 C157.143548,0.509206314 157.143548,0.509206314 157.001224,0.507006454 C156.826438,0.504389215 156.681281,0.502689027 156.544858,0.501793744 C156.379942,0.500968735 156.206042,0.50047943 156.008899,0.500230522 C155.73508,0.500000033 155.73508,0.500000033 155.462317,0.5 Z" id="Path"></path><path d="M89.3474827,18.6666318 C89.8655834,18.9767175 90.5368956,18.8045568 90.8449931,18.2823221 C91.1529511,17.7609293 90.9825765,17.086458 90.4630805,16.7763724 C86.1268307,14.1880695 79.2652415,13.9447715 75.1375994,15.2046171 C74.5597769,15.3811274 74.2340977,15.994704 74.4090771,16.5755884 C74.5840564,17.1561922 75.19467,17.4839569 75.7720738,17.3078675 C79.3678011,16.2105012 85.5808227,16.4173185 89.3474827,18.6666318 Z M89.2241321,21.9981581 C89.4874383,21.5679669 89.3526456,21.0039197 88.9244067,20.7387334 C85.317656,18.5099053 80.0417645,17.8827186 75.7976091,19.1776417 C75.3167648,19.3249675 75.0449469,19.8356967 75.1902048,20.320188 C75.3364395,20.803697 75.8453307,21.0768811 76.3271518,20.9305375 C80.0423227,19.7965502 84.8308117,20.3587733 87.9710904,22.2998251 C88.3993293,22.5644502 88.9599887,22.4293314 89.2241321,21.9981581 Z M87.7942971,25.1976523 C88.0043002,24.8524892 87.8961591,24.4013918 87.5527587,24.1905055 C84.4727599,22.298001 80.6544712,21.8575672 76.203243,22.8808497 C75.8104465,22.9707886 75.5656987,23.363517 75.6555605,23.7576484 C75.7450037,24.1516395 76.1355676,24.3987259 76.5275268,24.3083661 C80.5950285,23.3740402 84.0488467,23.7549825 86.7929801,25.4413712 C87.1359618,25.6523978 87.5844336,25.5437977 87.7942971,25.1976523 Z M82.4353811,8.25 C88.8891039,8.25 94.1206227,13.5105113 94.1206227,19.9998597 C94.1206227,26.489629 88.8891039,31.75 82.4353811,31.75 C75.9817979,31.75 70.75,26.489629 70.75,19.9998597 C70.75,13.5105113 75.9817979,8.25 82.4353811,8.25 Z M102.541677,19.0972439 C104.999759,19.698333 106.002193,20.6313961 106.002193,22.3184863 C106.002193,24.31468 104.485287,25.604552 102.137857,25.604552 C100.478484,25.604552 98.9409261,25.011601 97.6905356,23.8892594 C97.637093,23.8418345 97.6312324,23.7598934 97.6775587,23.7050321 L98.7821331,22.3831693 C98.8047381,22.3558088 98.8372502,22.3392522 98.8726926,22.3363057 C98.9064605,22.3343414 98.9430192,22.345005 98.9696707,22.3680158 C100.044245,23.3037448 101.004956,23.7026468 102.182509,23.7026468 C103.24285,23.7026468 103.901743,23.2413067 103.901743,22.4992059 C103.901743,21.8293648 103.575924,21.4548768 101.645291,21.0077081 C99.376979,20.4564292 98.0643551,19.7422501 98.0643551,17.7252905 C98.0643551,15.8444318 99.5860054,14.5298652 101.764456,14.5298652 C103.246059,14.5298652 104.513613,14.9714215 105.639257,15.8796496 C105.693816,15.9237071 105.70484,16.002842 105.664374,16.0603692 L104.679243,17.4567367 C104.658452,17.4862018 104.627335,17.5055646 104.592032,17.5108964 C104.55673,17.5165088 104.520869,17.5079499 104.492264,17.4864825 C103.546343,16.7773545 102.644376,16.4328929 101.734595,16.4328929 C100.79607,16.4328929 100.164805,16.8863756 100.164805,17.5605663 C100.164805,18.2739035 100.524112,18.6134543 102.541677,19.0972439 Z M113.017133,21.3583434 C113.017133,19.9566441 112.124376,18.9396756 110.894916,18.9396756 C109.659595,18.9396756 108.727768,19.9790938 108.727768,21.3583434 C108.727768,22.7377333 109.659595,23.7770112 110.894916,23.7770112 C112.144469,23.7770112 113.017133,22.7829132 113.017133,21.3583434 Z M111.313108,17.09642 C113.203694,17.09642 115.117583,18.5602767 115.117583,21.3583434 C115.117583,24.1558488 113.203694,25.6192846 111.313108,25.6192846 C110.295884,25.6192846 109.461174,25.2398857 108.772838,24.4631283 L108.772838,27.7228156 C108.772838,27.7961979 108.713674,27.8559701 108.640697,27.8559701 L106.834251,27.8559701 C106.761274,27.8559701 106.70225,27.7961979 106.70225,27.7228156 L106.70225,17.3957017 C106.70225,17.3223195 106.761274,17.2625473 106.834251,17.2625473 L108.640697,17.2625473 C108.713674,17.2625473 108.772838,17.3223195 108.772838,17.3957017 L108.772838,18.3251167 C109.461034,17.499391 110.295605,17.09642 111.313108,17.09642 Z M120.023422,23.7918841 C121.330883,23.7918841 122.279874,22.7816504 122.279874,21.3887907 C122.279874,20.0005612 121.296976,18.9542678 119.993561,18.9542678 C118.694612,18.9542678 117.75204,19.9653434 117.75204,21.3583434 C117.75204,22.7455907 118.72838,23.7918841 120.023422,23.7918841 Z M120.023422,17.09642 C122.450109,17.09642 124.350323,18.9684392 124.350323,21.3583434 C124.350323,23.7562453 122.436992,25.6351397 119.993561,25.6351397 C117.575526,25.6351397 115.681172,23.7697151 115.681172,21.3887907 C115.681172,18.9816283 117.588642,17.09642 120.023422,17.09642 Z M129.549748,17.2625473 C129.622726,17.2625473 129.681471,17.3223195 129.681471,17.3957017 L129.681471,18.9569337 C129.681471,19.0301757 129.622726,19.0899478 129.549748,19.0899478 L127.561766,19.0899478 L127.561766,22.8450706 C127.561766,23.438162 127.820467,23.7026468 128.399964,23.7026468 C128.776016,23.7026468 129.112439,23.6214072 129.460445,23.4453178 C129.500771,23.4252535 129.549748,23.4265162 129.589098,23.4513512 C129.628028,23.4754845 129.651889,23.5187001 129.651889,23.5645815 L129.651889,25.0513088 C129.651889,25.0987336 129.626214,25.1433523 129.585051,25.1665035 C129.078392,25.4554022 128.540757,25.5899598 127.892747,25.5899598 C126.299096,25.5899598 125.491037,24.7622697 125.491037,23.1297601 L125.491037,19.0899478 L124.62242,19.0899478 C124.549582,19.0899478 124.490837,19.0301757 124.490837,18.9569337 L124.490837,17.3957017 C124.490837,17.3223195 124.549582,17.2625473 124.62242,17.2625473 L125.491037,17.2625473 L125.491037,15.2187884 C125.491037,15.1454062 125.55048,15.0857743 125.623597,15.0857743 L127.429903,15.0857743 C127.502881,15.0857743 127.561766,15.1454062 127.561766,15.2187884 L127.561766,17.2625473 L129.549748,17.2625473 Z M136.475943,17.270545 L139.562081,17.270545 C139.61692,17.270545 139.666176,17.3045001 139.685432,17.3561342 L141.795649,22.893197 L143.722236,17.3597823 C143.740655,17.3064644 143.791167,17.270545 143.8474,17.270545 L145.728079,17.270545 C145.771894,17.270545 145.811941,17.2922931 145.836778,17.3282125 C145.861756,17.3646932 145.86636,17.4108552 145.850732,17.4518258 L142.716593,25.5954319 C142.06649,27.2763485 141.329177,27.9012903 139.995204,27.9012903 C139.28217,27.9012903 138.705046,27.7527017 138.065827,27.404592 C138.00471,27.3714788 137.979872,27.2965531 138.009035,27.232712 L138.621323,25.8819454 C138.635975,25.8481306 138.664999,25.8224538 138.699464,25.8101065 C138.734348,25.7993026 138.772442,25.8022491 138.804675,25.8200685 C139.147378,26.0086455 139.482545,26.1041968 139.800829,26.1041968 C140.194602,26.1041968 140.483025,25.9739889 140.776332,25.3194416 L138.192526,19.0899478 L136.505804,19.0899478 L136.505804,25.3211253 C136.505804,25.3945076 136.446361,25.4541395 136.373383,25.4541395 L134.566938,25.4541395 C134.4941,25.4541395 134.435215,25.3945076 134.435215,25.3211253 L134.435215,19.0899478 L133.567296,19.0899478 C133.494457,19.0899478 133.434736,19.0301757 133.434736,18.9569337 L133.434736,17.387704 C133.434736,17.3144621 133.494457,17.2546899 133.567296,17.2546899 L134.435215,17.2546899 L134.435215,16.8702399 C134.435215,15.0836697 135.317089,14.1388206 136.985811,14.1388206 C137.671496,14.1388206 138.129456,14.2488238 138.487368,14.3557402 C138.543322,14.3729983 138.580718,14.4249131 138.580718,14.4832822 L138.580718,16.0140668 C138.580718,16.0567212 138.561044,16.0969902 138.526159,16.1221057 C138.492252,16.1472213 138.448716,16.1538158 138.406995,16.1406267 C138.068199,16.0269754 137.765683,15.9517691 137.389073,15.9517691 C136.757529,15.9517691 136.475943,16.2810772 136.475943,17.01953 L136.475943,17.270545 Z M132.460628,17.2625473 C132.533467,17.2625473 132.592909,17.3223195 132.592909,17.3957017 L132.592909,25.3211253 C132.592909,25.3945076 132.533467,25.4541395 132.460628,25.4541395 L130.654044,25.4541395 C130.581066,25.4541395 130.521763,25.3945076 130.521763,25.3211253 L130.521763,17.3957017 C130.521763,17.3223195 130.581066,17.2625473 130.654044,17.2625473 L132.460628,17.2625473 Z M131.566476,13.6539084 C132.281881,13.6539084 132.862076,14.2366168 132.862076,14.956268 C132.862076,15.6761998 132.281881,16.2594694 131.566476,16.2594694 C130.85093,16.2594694 130.270038,15.6761998 130.270038,14.956268 C130.270038,14.2366168 130.85093,13.6539084 131.566476,13.6539084 Z M147.466988,18.0509505 L147.136007,18.0509505 L147.136007,18.475389 L147.466988,18.475389 C147.6322,18.475389 147.730853,18.3941494 147.730853,18.2629593 C147.730853,18.124894 147.6322,18.0509505 147.466988,18.0509505 Z M147.681596,18.6563892 L148.041183,19.1626285 L147.737969,19.1626285 L147.414243,18.698342 L147.136007,18.698342 L147.136007,19.1626285 L146.882189,19.1626285 L146.882189,17.8208417 L147.477314,17.8208417 C147.787226,17.8208417 147.991229,17.9802342 147.991229,18.248788 C147.991229,18.4687944 147.864808,18.6032116 147.681596,18.6563892 Z M147.399732,17.3742343 C146.748513,17.3742343 146.255668,17.8947852 146.255668,18.5322146 C146.255668,19.1692231 146.745024,19.6828989 147.393034,19.6828989 C148.044113,19.6828989 148.537376,19.1626285 148.537376,18.5249185 C148.537376,17.8877697 148.047741,17.3742343 147.399732,17.3742343 Z M147.393034,19.8107215 C146.678047,19.8107215 146.121713,19.2333449 146.121713,18.5322146 C146.121713,17.8309441 146.685303,17.2471132 147.399732,17.2471132 C148.114579,17.2471132 148.671053,17.8243495 148.671053,18.5249185 C148.671053,19.2260487 148.107882,19.8107215 147.393034,19.8107215 Z" id="Combined-Shape" fill="#000000"></path><path d="M19.609,22.904 L15.75,22.904 L15.75,16.8775 L16.923,16.8775 L16.923,21.782 L19.609,21.782 L19.609,22.904 Z M22.406419,22.904 L21.216419,22.904 L21.216419,16.8775 L22.406419,16.8775 L22.406419,22.904 Z M28.6378379,18.314 L27.5838379,18.637 C27.5243379,18.3055 27.2523379,17.787 26.4873379,17.787 C25.9178379,17.787 25.5438379,18.1525 25.5438379,18.552 C25.5438379,18.8835 25.7563379,19.147 26.1983379,19.232 L27.0398379,19.3935 C28.1363379,19.606 28.7228379,20.32 28.7228379,21.17 C28.7228379,22.0965 27.9493379,23.0315 26.5468379,23.0315 C24.9488379,23.0315 24.2433379,22.003 24.1498379,21.1445 L25.2378379,20.8555 C25.2888379,21.4505 25.7053379,21.986 26.5553379,21.986 C27.1843379,21.986 27.5328379,21.6715 27.5328379,21.2465 C27.5328379,20.898 27.2693379,20.626 26.8018379,20.5325 L25.9603379,20.3625 C24.9998379,20.167 24.3793379,19.5465 24.3793379,18.637 C24.3793379,17.566 25.3398379,16.75 26.4788379,16.75 C27.9408379,16.75 28.5018379,17.634 28.6378379,18.314 Z M34.9882569,17.991 L33.0842569,17.991 L33.0842569,22.904 L31.9027569,22.904 L31.9027569,17.991 L29.9987569,17.991 L29.9987569,16.8775 L34.9882569,16.8775 L34.9882569,17.991 Z M40.3696759,22.904 L36.5956759,22.904 L36.5956759,16.8775 L40.3696759,16.8775 L40.3696759,17.9825 L37.7686759,17.9825 L37.7686759,19.3595 L40.1231759,19.3595 L40.1231759,20.405 L37.7686759,20.405 L37.7686759,21.799 L40.3696759,21.799 L40.3696759,22.904 Z M47.4170948,22.904 L46.1845948,22.904 L43.4475948,18.6285 L43.4475948,22.904 L42.2745948,22.904 L42.2745948,16.8775 L43.7365948,16.8775 L46.2440948,20.864 L46.2440948,16.8775 L47.4170948,16.8775 L47.4170948,22.904 Z M53.1054328,19.8865 C53.1054328,21.2465 54.0574328,21.8925 54.9839328,21.8925 C55.9189328,21.8925 56.8709328,21.2465 56.8709328,19.8865 C56.8709328,18.5265 55.9189328,17.8805 54.9839328,17.8805 C54.0574328,17.8805 53.1054328,18.5265 53.1054328,19.8865 Z M51.8899328,19.895 C51.8899328,17.9485 53.3519328,16.75 54.9839328,16.75 C56.6244328,16.75 58.0864328,17.9485 58.0864328,19.895 C58.0864328,21.833 56.6244328,23.0315 54.9839328,23.0315 C53.3519328,23.0315 51.8899328,21.833 51.8899328,19.895 Z M65.0488517,22.904 L63.8163517,22.904 L61.0793517,18.6285 L61.0793517,22.904 L59.9063517,22.904 L59.9063517,16.8775 L61.3683517,16.8775 L63.8758517,20.864 L63.8758517,16.8775 L65.0488517,16.8775 L65.0488517,22.904 Z" id="LISTEN-ON" fill="#000000"></path></g></g></svg></a></li></ul></div></span></div><div class="LWPostsPageHeader-titleSection LWPostsPageHeader-titleSectionWithSplashPageHeader"><div class="LWPostsPageHeader-title"><div><h1 class="Typography-root Typography-display3 PostsPageTitle-root LWPostsPageHeader-splashPageTitle"><a class="PostsPageTitle-link" href="/posts/vJFdjigzmcXMhNTsx/simulators"><span class="PostsPageTitle-lastWord">Simulators</span></a></h1></div><div class="LWPostsPageHeader-authorAndSecondaryInfo"><div class="LWPostsPageHeader-authorInfo"><span class="Typography-root Typography-body1 PostsAuthors-root">by <span class="PostsAuthors-authorName"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/janus-1?from=post_header">janus</a></span></span></span></span></div><div class="LWPostsPageHeader-date"><span class="LWTooltip-root"><span class="PostsPageDate-date"><time dateTime="2022-09-02T12:45:33.723Z">2nd Sep 2022</time></span></span></div><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators">AI Alignment Forum</a></span><span class="LWTooltip-root"><a href="https://generative.ink/posts/simulators/" target="_blank">Linkpost from <!-- -->generative.ink</a></span><div class="LWPostsPageHeader-mobileButtons"><div class="LWPostsPageHeader-readTime"><span class="LWTooltip-root"><span class="ReadTime-root">49<!-- --> min read</span></span></div><div class="LWCommentCount-root"><a class="LWCommentCount-comments LWCommentCount-wideClickTarget" href="#comments"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="LWCommentCount-commentsIcon ForumIcon-root"><path stroke-linecap="round" stroke-linejoin="round" d="M2.25 12.76c0 1.6 1.123 2.994 2.707 3.227 1.087.16 2.185.283 3.293.369V21l4.076-4.076a1.526 1.526 0 011.037-.443 48.282 48.282 0 005.68-.494c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0012 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741v6.018z"></path></svg>169</a></div><div class="LWPostsPageHeader-audioToggle"><span class="LWTooltip-root AudioToggle-togglePodcastContainer"><a href="#"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true" class="AudioToggle-audioIcon ForumIcon-root"><g stroke-width="1.5" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" d="M19.114 5.636a9 9 0 010 12.728M16.463 8.288a5.25 5.25 0 010 7.424M6.75 8.25l4.72-4.72a.75.75 0 011.28.53v15.88a.75.75 0 01-1.28.53l-4.72-4.72H4.51c-.88 0-1.704-.507-1.938-1.354A9.01 9.01 0 012.25 12c0-.83.112-1.633.322-2.396C2.806 8.756 3.63 8.25 4.51 8.25H6.75z"></path></g></svg></a></span></div><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div></div></div><div class="LWPostsPageHeader-mobileHeaderVote"><div class="PostsVoteDefault-voteBlock"><div class="PostsVoteDefault-upvote"><button class="VoteButton-root VoteArrowIconHollow-up" type="button"><span class="VoteButton-inner"><svg class="VoteArrowIconHollow-smallArrow" viewBox="6 6 12 12"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></div><div class="PostsVoteDefault-voteScores"><span><h1 class="Typography-root Typography-headline PostsVoteDefault-voteScore">690</h1></span><span><h1 class="Typography-root Typography-headline PostsVoteDefault-voteScore PostsVoteDefault-secondaryVoteScore">Ω <!-- -->142</h1></span></div><div class="PostsVoteDefault-downvote"><button class="VoteButton-root VoteArrowIconHollow-down" type="button"><span class="VoteButton-inner"><svg class="VoteArrowIconHollow-smallArrow" viewBox="6 6 12 12"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></div></div></div></div><!--$?--><template id="B:6"></template><!--/$--></div></div></div><div class="PostsPage-postContent instapaper_body ContentStyles-base content ContentStyles-postBody"><div class="commentOnSelection"><div id="postContent"><div><div><p id="block0"><i>Thanks to Chris Scammell, Adam Shimi, Lee Sharkey, Evan Hubinger, Nicholas Dupuis, Leo Gao, Johannes Treutlein, and Jonathan Low for feedback on drafts.</i></p><p id="block1"><i>This work was carried out while at</i><span><span><a href="https://www.conjecture.dev/"><i> Conjecture</i></a></span></span><i>.</i></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/mal6l24rfuoinreeemia.png" srcSet="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094430/mirroredImages/vJFdjigzmcXMhNTsx/ittjfpf5suacww8pufdj.png 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/tdi9oceca4v15gnjbgzx.png 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/szo4z9os5qpka2wdcxt4.png 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/fktu9hq9bxiz3riagkvh.png 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094430/mirroredImages/vJFdjigzmcXMhNTsx/auhvstkeaettn98iorqc.png 1250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094430/mirroredImages/vJFdjigzmcXMhNTsx/ctwytv1yywj8z9f5uwq3.png 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094430/mirroredImages/vJFdjigzmcXMhNTsx/ylccpb8anselhpwphiwd.png 1750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/ojbbpp8ltpbsy28nnhqa.png 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/qxnpbhmhg2b2bb9bz013.png 2250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/ytdyqqynryhcq1ysqbtk.png 2496w"/></figure><p id="block2"><i>&quot;Moebius illustration of a simulacrum living in an AI-generated story discovering it is in a simulation&quot; by DALL-E 2</i></p><h2 id="Summary">Summary</h2><p id="block3"><strong>TL;DR</strong>: Self-supervised learning may create AGI or its foundation. What would that look like?</p><p id="block4">Unlike the limit of RL, the limit of self-supervised learning has received surprisingly little conceptual attention, and recent progress has made deconfusion in this domain more pressing.</p><p id="block5">Existing AI taxonomies either fail to capture important properties of self-supervised models or lead to confusing propositions. For instance, GPT policies do not seem globally agentic, yet can be conditioned to behave in goal-directed ways. This post describes a frame that enables more natural reasoning about properties like agency: GPT, insofar as it is inner-aligned, is a <strong>simulator</strong> which can simulate agentic and non-agentic <strong>simulacra</strong>.</p><p id="block6"><span class="blockquote_PpymDqWWxttapcqXA_1">The purpose of this post is to capture these objects in words </span><s><span class="blockquote_PpymDqWWxttapcqXA_1">so GPT can reference them</span></s><span class="blockquote_PpymDqWWxttapcqXA_1"> and provide a better foundation for understanding them.</span></p><span class="blockquote_PpymDqWWxttapcqXA_1"></span><p id="block7">I use the generic term “simulator” to refer to models trained with predictive loss on a self-supervised dataset, invariant to architecture or data type (natural language, code, pixels, game states, etc). The outer objective of self-supervised learning is Bayes-optimal conditional inference over the prior of the training distribution, which I call the <strong>simulation objective</strong>, because a conditional model can be used to simulate rollouts which probabilistically obey its learned distribution by iteratively sampling from its posterior (predictions) and updating the condition (prompt). Analogously, a predictive model of physics can be used to compute rollouts of phenomena in simulation. A goal-directed agent which evolves according to physics can be simulated by the physics rule parameterized by an initial state, but the same rule could also propagate agents with different values, or non-agentic phenomena like rocks. This ontological distinction between simulator (rule) and simulacra (phenomena) applies directly to generative models like GPT.</p><h2 id="Meta">Meta</h2><ul><li id="block8">This post is intended as the first in a sequence on the alignment problem in a landscape where self-supervised simulators are a possible/likely form of powerful AI. I don’t know how many subsequent posts I’ll actually publish. Take it as a prompt.</li><li id="block9">I use the generic term “GPT” to refer to transformers trained on next-token prediction.</li><li id="block10">A while ago when I was trying to avoid having to write this post by hand, I prompted GPT-3 with an early outline of this post. I’ve spliced in some excerpts from it, <code>indicated by this style</code>. Prompt, generated text, and curation metrics <span><span><a href="https://generative.ink/artifacts/simulators/">here</a></span></span>.</li></ul><h1 id="The_limit_of_sequence_modeling">The limit of sequence modeling</h1><blockquote id="block11"><p id="block12">Transformer-based language models have recently achieved remarkable results…</p><p id="block13">– every paper since 2020</p></blockquote><hr/><p id="block14"><span></span>GPT is not a new form of AI in terms of its training methodology and outer objective: sequence generation from statistical models of data is an old idea. In 1951, Claude Shannon described using <span><span><a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a></span></span> to approximate conditional next-letter probabilities of a text dataset and &quot;reversed&quot; to generate text samples<span role="doc-noteref" id="fnrefdbsg8o0p25q" class="footnote-reference"><sup><span><a href="#fndbsg8o0p25q" class="">[1]</a></span></sup></span>. <span class="blockquote_KbhyCWNSGPj3oJz5g_1">I don&#x27;t know of any other notable advances until the 2010s brought the first interesting language generation results from neural networks.</span> In 2015, Karpathy wrote a blog post/tutorial sharing his excitement about <span><span><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></span></span>:</p><blockquote id="block15"><p id="block16">Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.</p><p id="block17">We’ll train RNNs to generate text character by character and ponder the question “how is that even possible?&quot;</p></blockquote><p id="block18">The “magical outputs” of char-RNNs looked like this:</p><blockquote id="block19"><p id="block20">PANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain’d into being never fed, And who is but a chain and subjects of his death, I should not sleep.</p><p id="block21">Second Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.</p><p id="block22">DUKE VINCENTIO: Well, your wit is in the care of side and that.</p><p id="block23">Second Lord: They would be ruled after this chamber, and my fair nues begun out of the fact, to be conveyed, Whose noble souls I’ll have the heart of the wars.</p><p id="block24">Clown: Come, sir, I will make did behold your worship.</p><p id="block25">VIOLA: I’ll drink it.</p></blockquote><p id="block26">At the time, this really was magical (and <span><span><a href="https://en.wikipedia.org/wiki/Uncanny_valley">uncanny</a></span></span>). How does it know that <i>miseries</i> are <i>produced upon the soul?</i> Or that a <i>clown</i> should address a <i>lord</i> as “sir”? Char-RNNs were like ouija boards, but actually possessed by a low-fidelity ghost summoned from a text corpus. I remember being thrilled by the occasional glimmers of semantic comprehension in a domain of unbounded constructive meaning.</p><p id="block27">But, aside from indulging that emotion, I didn’t think about what would happen if my char-RNN bots actually improved indefinitely at their training objective of natural language prediction. It just seemed like there were some complexity classes of magic that neural networks could learn, and others that were inaccessible, at least in the conceivable future.</p><p id="block28">Huge mistake! Perhaps I could have started thinking several years earlier about what now seems so fantastically important. But it wasn’t until GPT-3, when I saw the <span><span><a href="https://www.gwern.net/GPT-3">qualitative correlate</a></span></span> of “loss going down”, that I updated.</p><p id="block29">I wasn’t the only one<span role="doc-noteref" id="fnrefa35qx2ldayo" class="footnote-reference"><sup><span><a href="#fna35qx2ldayo" class="">[2]</a></span></sup></span> whose imagination was naively constrained. A 2016 paper from Google Brain, “<span><span><a href="https://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></span></span>”, describes the utility of training language models as follows:</p><blockquote id="block30"><p id="block31">Often (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.</p></blockquote><p id="block32">Despite its title, this paper’s analysis is entirely myopic. Improving BLEU scores is neat, but how about <i>modeling general intelligence</i> as a downstream task? <span><span><a href="https://arxiv.org/abs/2005.14165">In</a></span></span> <span><span><a href="https://arxiv.org/abs/2204.02311">retrospect</a></span></span>, an exploration of the <i>limits</i> of language modeling should have read something more like:</p><blockquote id="block33"><p id="block34">If loss keeps going down on the test set, in the limit – putting aside whether the current paradigm can approach it – the model must be learning to interpret and predict all patterns represented in language, including common-sense reasoning, goal-directed optimization, and deployment of the sum of recorded human knowledge. Its outputs would behave as intelligent entities in their own right. You could converse with it by alternately generating and adding your responses to its prompt, and it would pass the Turing test. In fact, you could condition it to generate interactive and autonomous versions of any real or fictional person who has been recorded in the training corpus or even <i>could</i> be recorded (in the sense that the record counterfactually “could be” in the test set). Oh shit, and it could write code…</p></blockquote><p id="block35">The paper does, however, mention that making the model bigger improves test perplexity.<span role="doc-noteref" id="fnrefhxtnxj1c2hb" class="footnote-reference"><sup><span><a href="#fnhxtnxj1c2hb" class="">[3]</a></span></sup></span></p><p id="block36">I’m only picking on <i>Jozefowicz et al.</i> because of their ironic title. I don’t know of any explicit discussion of this limit predating GPT, except a working consensus of Wikipedia editors that <span><span><a href="https://en.wikipedia.org/wiki/Natural-language_understanding">NLU</a></span></span> is <span><span><a href="https://en.wikipedia.org/wiki/AI-complete#AI-complete_problems">AI-complete</a></span></span>.</p><p id="block37">The earliest engagement with the hypothetical of “<i>what if self-supervised sequence modeling actually works</i>” that I know of is a terse post from 2019, <a class="LinkStyles-link" href="/posts/YJRb6wRHp7k39v69n/implications-of-gpt-2">Implications of GPT-2</a>, by Gurkenglas. It is brief and relevant enough to quote in full:</p><blockquote id="block38"><p id="block39">I was impressed by GPT-2, to the point where I wouldn’t be surprised if a future version of it could be used pivotally using existing protocols.</p><p id="block40">Consider generating half of a Turing test transcript, the other half being supplied by a human judge. If this passes, we could immediately implement an HCH of AI safety researchers solving the problem if it’s within our reach at all. (Note that training the model takes much more compute than generating text.)</p><p id="block41">This might not be the first pivotal application of language models that becomes possible as they get stronger.</p><p id="block42">It’s a source of superintelligence that doesn’t automatically run into utility maximizers. It sure doesn’t look like AI services, lumpy or no.</p></blockquote><p id="block43">It is conceivable that predictive loss does not descend to the AGI-complete limit, maybe because:</p><ul><li id="block44">Some AGI-necessary predictions are <a class="LinkStyles-link" href="/posts/pv7Qpu8WSge8NRbpB">too difficult to be learned by even a scaled version of the current paradigm</a>.</li><li id="block45">The irreducible entropy is above the “AGI threshold”: datasets + context windows <span><span><a href="https://twitter.com/ylecun/status/1562162165540331520">contain insufficient information</a></span></span> to improve on some necessary predictions.</li></ul><p id="block46">But I have not seen enough evidence for either not to be concerned that we have in our hands a well-defined protocol that could end in AGI, or a foundation which could spin up an AGI without too much additional finagling. As Gurkenglas observed, this would be a very different source of AGI than previously foretold.</p><h1 id="The_old_framework_of_alignment">The old framework of alignment</h1><p id="block47">A few people did think about what would happen if <i>agents</i> actually worked. The hypothetical limit of a powerful system <strong>optimized to optimize for an objective</strong> drew attention even before reinforcement learning became mainstream in the 2010s. Our current instantiation of AI alignment theory, <a class="LinkStyles-link" href="/posts/i4susk4W3ieR5K92u/ai-risk-and-opportunity-humanity-s-efforts-so-far">crystallized by Yudkowsky, Bostrom, et al</a>, stems from the vision of an arbitrarily-capable system whose cognition and behavior flows from a goal.</p><p id="block48">But since GPT-3 I’ve <a class="LinkStyles-link" href="/s/zpCiuR4T343j9WkcK/p/5JDkW4MYXit2CquLs">noticed</a>, in my own thinking and in alignment discourse, a dissonance between theory and practice/phenomena, as the behavior and nature of actual systems that seem nearest to AGI also resist <i>short descriptions in the dominant ontology</i>.</p><p id="block49">I only recently discovered the question “<a class="LinkStyles-link" href="/posts/dPcKrfEi87Zzr7w6H/is-the-work-on-ai-alignment-relevant-to-gpt">Is the work on AI alignment relevant to GPT?</a>” which stated this observation very explicitly:</p><blockquote id="block50"><p id="block51">I don’t follow [AI alignment research] in any depth, but I am noticing a striking disconnect between the concepts appearing in those discussions and recent advances in AI, especially GPT-3.</p><p id="block52">People talk a lot about an AI’s goals, its utility function, its capability to be deceptive, its ability to simulate you so it can get out of a box, ways of motivating it to be benign, Tool AI, Oracle AI, and so on. (…) But when I look at GPT-3, even though this is already an AI that Eliezer finds alarming, I see none of these things. GPT-3 is a huge model, trained on huge data, for predicting text.</p></blockquote><p id="block53">My belated answer: A lot of prior work on AI alignment is relevant to GPT. I spend most of my time thinking about GPT alignment, and concepts like <span class=""><a class="LinkStyles-link" href="/w/goal-directedness">goal-directedness</a></span>, <span class=""><a class="LinkStyles-link" id="r9tYkB2a8Fp4DN8yB" href="/s/r9tYkB2a8Fp4DN8yB">inner/outer alignment</a></span>, <span class=""><a class="LinkStyles-link" href="/w/myopia">myopia</a></span>, <span class=""><a class="LinkStyles-link" href="/w/corrigibility">corrigibility</a></span>, <a class="LinkStyles-link" href="/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version">embedded agency</a>, <a class="LinkStyles-link" href="/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1">model splintering</a>, and even <span class=""><a class="LinkStyles-link" href="/w/tiling_agents">tiling agents</a></span> are active in the vocabulary of my thoughts. But GPT violates some prior assumptions such that these concepts sound dissonant when applied naively. To usefully harness these preexisting abstractions, we need something like an ontological <span><span><a href="https://en.wikipedia.org/wiki/Adapter_pattern">adapter pattern</a></span></span> that maps them to the appropriate objects.</p><p id="block54">GPT’s unforeseen nature also demands new abstractions (the adapter itself, for instance). My thoughts also use load-bearing words that do not inherit from alignment literature. Perhaps it shouldn’t be surprising if the form of the first visitation from <a class="LinkStyles-link" href="/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general">mindspace</a> mostly escaped a few years of theory <a class="LinkStyles-link" href="/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment#Direct_access__so_far_and_yet_so_close">conducted in absence of its object</a>.</p><p id="block55">The purpose of this post is to capture that object (conditional on a predictive self-supervised training story) in words. Why in words? In order to write coherent alignment ideas which reference it! This is difficult in the existing ontology, because unlike the concept of an <i>agent</i>, whose <i>name</i> evokes the abstract properties of the system and thereby invites extrapolation, the general category for “a model optimized for an AGI-complete predictive task” has not been given a name<span role="doc-noteref" id="fnrefx2xlz0klyh8" class="footnote-reference"><sup><span><a href="#fnx2xlz0klyh8" class="">[4]</a></span></sup></span>. Namelessness can not only be a symptom of the extrapolation of powerful predictors falling through conceptual cracks, but also a cause, because what we can represent in words is <i>what we can condition on for further generation.</i> To whatever extent this <span><span><a href="https://en.wikipedia.org/wiki/Language_of_thought_hypothesis">shapes private thinking</a></span></span>, it is a strict constraint on communication, when thoughts must be sent through the bottleneck of words.</p><p id="block56">I want to hypothesize about LLMs in the limit, because when AI is all of a sudden <span><span><a href="https://www.theverge.com/2020/8/16/21371049/gpt3-hacker-news-ai-blog">writing viral blog posts</a></span></span>, <span><span><a href="https://www.deepmind.com/blog/competitive-programming-with-alphacode">coding competitively</a></span></span>, <span><span><a href="https://arxiv.org/abs/2009.03393">proving theorems</a></span></span>, and <span><span><a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">passing the Turing test so hard that the interrogator sacrifices their career at Google to advocate for its personhood</a></span></span>, a process is clearly underway whose limit we’d be foolish not to contemplate. I could directly extrapolate the architecture responsible for these feats and talk about “GPT-N”, a bigger autoregressive transformer. But often some implementation details aren’t as important as the more abstract archetype that GPT represents – I want to speak the <a class="LinkStyles-link" href="/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation">true name</a> of the solution which unraveled a Cambrian explosion of AI phenomena with <i>inessential details unconstrained</i>, as we’d speak of natural selection finding the solution of the “lens” without specifying the prototype’s diameter or focal length.</p><p id="block57">(Only when I am able to condition on that level of abstraction can I generate metaphors like “language is a <a class="LinkStyles-link" href="/s/5g5TkQTe9rmPS5vvM/p/46qnWRSR7L2eyNbMA">lens that sees its flaws</a>”.)</p><h1 id="Inadequate_ontologies">Inadequate ontologies</h1><p id="block58">In the next few sections I’ll attempt to fit GPT into some established categories, hopefully to reveal something about the shape of the peg through contrast, beginning with the main antagonist of the alignment problem as written so far, the <strong>agent</strong>.</p><h2 id="Agentic_GPT">Agentic GPT</h2><p id="block59">Alignment theory has been largely pushed by considerations of agentic AGIs. There were good reasons for this focus:</p><ul><li id="block60"><strong>Agents are convergently dangerous</strong> <strong>for theoretical reasons</strong> like <span class=""><a class="LinkStyles-link" href="/w/instrumental-convergence">instrumental convergence</a></span>, <span class=""><a class="LinkStyles-link" href="/w/goodhart-s-law">goodhart</a></span>, and <span class=""><a class="LinkStyles-link" href="/w/orthogonality-thesis">orthogonality</a></span>.</li><li id="block61"><span></span><strong>RL creates agents, and RL seemed to be the way to AGI</strong>. In the 2010s, reinforcement learning was the dominant paradigm for those interested in AGI (e.g. OpenAI). RL lends naturally to creating agents that pursue rewards/utility/objectives. So there was reason to expect that agentic AI would be the first (and by the theoretical arguments, last) form that superintelligence would take.</li><li id="block62"><strong>Agents are powerful and economically productive.</strong> It’s a reasonable guess that humans will create such systems <span><span><a href="https://mittmattmutt.medium.com/superintelligence-and-moral-blindness-7436300fcb1f">if only because we can</a></span></span>.</li></ul><p id="block63">The first reason is conceptually self-contained and remains compelling. The second and third, grounded in the state of the world, has been shaken by the current climate of AI progress, where products of self-supervised learning generate most of the buzz: not even primarily for their SOTA performance in domains traditionally dominated by RL, like games<span role="doc-noteref" id="fnref3ifsldmwtxr" class="footnote-reference"><sup><span><a href="#fn3ifsldmwtxr" class="">[5]</a></span></sup></span>, but rather for their virtuosity in domains where RL never even took baby steps, like natural language synthesis.</p><p id="block64">What pops out of self-supervised predictive training is noticeably not a classical agent. Shortly after GPT-3’s release, David Chalmers lucidly observed that the policy’s relation to agent<i>s</i> is like that of a “chameleon” or “engine”:</p><blockquote id="block65"><p id="block66">GPT-3 does not look much like an agent. It does not seem to have goals or preferences beyond completing text, for example. It is more like a chameleon that can take the shape of many different agents. Or perhaps it is an engine that can be used under the hood to drive many agents. But it is then perhaps these systems that we should assess for agency, consciousness, and so on.<span role="doc-noteref" id="fnrefoti8ojhy48a" class="footnote-reference"><sup><span><a href="#fnoti8ojhy48a" class="">[6]</a></span></sup></span></p></blockquote><p id="block67">But at the same time, GPT can <i>act like an agent</i> – and aren’t actions what ultimately matter? In <a class="LinkStyles-link" href="/posts/kpPnReyBC54KESiSn">Optimality is the tiger, and agents are its teeth</a>, Veedrac points out that a model like GPT does not need to care about the consequences of its actions for them to be effectively those of an agent that kills you. This is <i>more</i> reason to examine the nontraditional relation between the optimized policy and agents, as it has implications for how and why agents are served.</p><h3 id="Unorthodox_agency">Unorthodox agency</h3><p id="block68"><code>GPT’s behavioral properties include imitating the general pattern of human dictation found in its universe of training data, e.g., arXiv, fiction, blog posts, Wikipedia, Google queries, internet comments, etc. Among other properties inherited from these historical sources, it is capable of goal-directed behaviors such as planning. For example, given a free-form prompt like, “you are a desperate smuggler tasked with a dangerous task of transporting a giant bucket full of glowing radioactive materials across a quadruple border-controlled area deep in Africa for Al Qaeda,” the AI will fantasize about logistically orchestrating the plot just as one might, working out how to contact Al Qaeda, how to dispense the necessary bribe to the first hop in the crime chain, how to get a visa to enter the country, etc. Considering that no such specific chain of events are mentioned in any of the bazillions of pages of unvarnished text that GPT slurped</code><span role="doc-noteref" id="fnref8p9svyjn8cv" class="footnote-reference"><sup><span><a href="#fn8p9svyjn8cv" class="">[7]</a></span></sup></span><code>, the architecture is not merely imitating the universe, but reasoning about possible versions of the universe that does not actually exist, branching to include new characters, places, and events</code></p><p id="block69"><code>When thought about behavioristically, GPT superficially demonstrates many of the raw ingredients to act as an “agent”, an entity that optimizes with respect to a goal. But GPT is hardly a proper agent, as it wasn’t optimized to achieve any particular task, and does not display an epsilon optimization for any single reward function, but instead for many, including incompatible ones. Using it as an agent is like using an agnostic politician to endorse hardline beliefs– he can convincingly talk the talk, but there is no psychic unity within him; he could just as easily play devil’s advocate for the opposing party without batting an eye. Similarly, GPT instantiates simulacra of characters with beliefs and goals, but none of these simulacra are the algorithm itself. They form a virtual procession of different instantiations as the algorithm is fed different prompts, supplanting one surface personage with another. Ultimately, the computation itself is more like a disembodied dynamical law that moves in a pattern that broadly encompasses the kinds of processes found in its training data than a cogito meditating from within a single mind that aims for a particular outcome.</code></p><p id="block70">Presently, GPT is the only way to instantiate agentic AI that behaves capably <span class=""><a class="LinkStyles-link" href="/w/rich_domain">outside toy domains</a></span>. These intelligences exhibit goal-directedness; they can plan; they can form and test hypotheses; they can persuade and be persuaded<span role="doc-noteref" id="fnref5z0xgsu2zo5" class="footnote-reference"><sup><span><a href="#fn5z0xgsu2zo5" class="">[8]</a></span></sup></span>. It would not be very <a class="LinkStyles-link" href="/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy">dignified</a> of us to gloss over the sudden arrival of artificial agents <i>often indistinguishable from human intelligence</i> just because the policy that generates them “only cares about predicting the next word”.</p><p id="block71">But nor should we ignore the fact that these agentic entities exist in an unconventional relationship to the policy, the neural network “GPT” that was trained to minimize log-loss on a dataset. GPT-driven agents are ephemeral – they can spontaneously disappear if the scene in the text changes and be replaced by different spontaneously generated agents. They can exist in parallel, e.g. in a story with multiple agentic characters in the same scene. There is a clear sense in which the network doesn’t “want” what the things that it simulates want, seeing as it would be just as willing to simulate an agent with opposite goals, or throw up obstacles which foil a character’s intentions for the sake of the story. The more you think about it, the more fluid and intractable it all becomes. Fictional characters act agentically, but they’re at least implicitly puppeteered by a virtual author who has orthogonal intentions of their own. Don’t let me get into the fact that all these layers of “intentionality” operate largely in <span><span><a href="https://generative.ink/posts/language-models-are-multiverse-generators/#multiplicity-of-pasts-presents-and-futures">indeterminate superpositions</a></span></span>.</p><p id="block72">This is a clear way that GPT diverges from orthodox visions of agentic AI: <strong>In the agentic AI ontology, there is no difference between the policy and the effective agent, but for GPT, there is.</strong></p><p id="block73">It’s not that anyone ever said there had to be 1:1 correspondence between policy and effective agent; it was just an implicit assumption which felt natural in the agent frame (for example, it tends to hold for RL). GPT pushes us to realize that this was an assumption, and to consider the consequences of removing it for our constructive maps of mindspace.</p><h3 id="Orthogonal_optimization">Orthogonal optimization</h3><p id="block74">Indeed, <a class="LinkStyles-link" href="/posts/8HWGXhnCfAPgJYa9D/pitfalls-of-the-agent-model">Alex Flint warned</a> of the potential consequences of leaving this assumption unchallenged:</p><blockquote id="block75"><p id="block76"><strong>Fundamental misperception due to the agent frame</strong>: That the design space for autonomous machines that exert influence over the future is narrower than it seems. This creates a self-fulfilling prophecy in which the AIs actually constructed are in fact within this narrower regime of agents containing an unchanging internal decision algorithm.</p></blockquote><p id="block77">If there are other ways of constructing AI, might we also avoid some of the scary, theoretically hard-to-avoid side-effects of optimizing an agent like <span class=""><a class="LinkStyles-link" href="/w/instrumental-convergence">instrumental convergence</a></span>? GPT provides an interesting example.</p><p id="block78">GPT doesn’t seem to care which agent it simulates, nor if the scene ends and the agent is effectively destroyed. This is not corrigibility in <span><span><a href="https://ai-alignment.com/corrigibility-3039e668638">Paul Christiano’s formulation</a></span></span>, where the policy is “okay” with being turned off or having its goal changed in a positive sense, but has many aspects of the <span class=""><a class="LinkStyles-link" href="/w/corrigibility">negative formulation found on Arbital</a></span>. It is corrigible in this way because a major part of the agent specification (the prompt) is not fixed by the policy, and the policy lacks direct training incentives to control its prompt<span role="doc-noteref" id="fnrefn3u1lofwts9" class="footnote-reference"><sup><span><a href="#fnn3u1lofwts9" class="">[9]</a></span></sup></span>, as it never generates text or otherwise influences its prompts during training. It’s <i>we</i> who choose to sample tokens from GPT’s predictions and append them to the prompt at runtime, and the result is not always helpful to any agents who may be programmed by the prompt. The downfall of the ambitious villain from an oversight committed in hubris is a predictable narrative pattern.<span role="doc-noteref" id="fnrefaxtq4tiuhug" class="footnote-reference"><sup><span><a href="#fnaxtq4tiuhug" class="">[10]</a></span></sup></span> So is the end of a scene.</p><p id="block79">In general, the model’s prediction vector could point in any direction relative to the predicted agent’s interests. <span class=""><span></span>I call this the </span><strong><span class="">prediction orthogonality thesis:</span></strong><span class=""> </span><i><span class="">A model whose objective is prediction</span></i><span role="doc-noteref" id="fnrefzjj9iwtc3or" class="footnote-reference"><sup><span><a href="#fnzjj9iwtc3or" class=""><span class="">[11]</span></a></span></sup></span><i><span class=""> can simulate agents who optimize toward any objectives, with any degree of optimality (bounded above but not below by the model’s power).</span></i></p><p id="block80">This is a corollary of the classical <span class=""><a class="LinkStyles-link" href="/w/orthogonality-thesis">orthogonality thesis</a></span>, which states that agents can have any combination of intelligence level and goal, combined with the assumption that agents can in principle be predicted. A single predictive model may also predict multiple agents, either independently (e.g. in different conditions), or interacting in a multi-agent simulation. A more optimal predictor is not restricted to predicting more optimal agents: being smarter does not make you unable to predict stupid systems, nor things that aren’t agentic like the <span><span><a href="https://en.wikipedia.org/wiki/History_of_numerical_weather_prediction">weather</a></span></span>.</p><p id="block81">Are there any constraints on what a predictive model can be at all, other than computability? Only that it makes sense to talk about its “prediction objective”, which implies the existence of a “ground truth” distribution to which the predictor’s optimality is measured. Several words in that last sentence may conceal labyrinths of nuance, but for now let’s wave our hands and say that if we have some way of presenting <a class="LinkStyles-link" href="/posts/QrhAeKBkm2WsdRYao/searching-for-bayes-structure">Bayes-structure</a> with evidence of a distribution, we can build an optimization process whose outer objective is optimal prediction.</p><p id="block82">We can specify some types of outer objectives using a ground truth distribution that we cannot with a utility function. As in the case of GPT, there is no difficulty in incentivizing a model to <i>predict</i> actions that are <span class=""><a class="LinkStyles-link" href="/w/corrigibility">corrigible</a></span>, <span><span><a href="https://aiimpacts.org/what-do-coherence-arguments-imply-about-the-behavior-of-advanced-ai/">incoherent</a></span></span>, <a class="LinkStyles-link" href="/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty">stochastic</a>, <a class="LinkStyles-link" href="/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality">irrational</a>, or otherwise anti-natural to expected utility maximization. All you need is evidence of a distribution exhibiting these properties.</p><p id="block83">For instance, during GPT’s training, sometimes predicting the next token coincides with predicting agentic behavior, but:</p><ul><li id="block84">The actions of agents described in the data are rarely optimal for their goals; humans, for instance, are computationally bounded, irrational, normative, habitual, fickle, hallucinatory, etc.</li><li id="block85">Different prediction steps involve mutually incoherent goals, as human text records a wide range of differently-motivated agentic behavior</li><li id="block86">Many prediction steps don’t correspond to the action of <i>any</i> consequentialist agent but are better described as reporting on the structure of reality, e.g. the year in a timestamp. These transitions incentivize GPT to improve its model of the world, orthogonally to agentic objectives.</li><li id="block87">When there is insufficient information to predict the next token with certainty, <span><span><a href="https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules">log-loss incentivizes a probabilistic output</a></span></span>. Utility maximizers <a class="LinkStyles-link" href="/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty">aren’t supposed to become more stochastic</a> in response to uncertainty.</li></ul><p id="block88">Everything can be trivially modeled as a utility maximizer, but for these reasons, a utility function is not a good explanation or compression of GPT’s training data, and its optimal predictor is not well-described as a utility maximizer. However, just because information isn’t compressed well by a utility function doesn’t mean it can’t be compressed another way. The <span><span><a href="https://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot set</a></span></span> is a complicated pattern compressed by a very simple generative algorithm which makes no reference to future consequences and doesn’t involve argmaxxing anything (except vacuously <a class="LinkStyles-link" href="/posts/d2n74bwham8motxyX/optimization-at-a-distance#An_Agent_Optimizing_Its_Own_Actions">being the way it is</a>). Likewise the set of all possible rollouts of <span><span><a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a></span></span> – <a class="LinkStyles-link" href="/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life">some automata may be well-described as agents</a>, but they are a minority of possible patterns, and not all agentic automata will share a goal. Imagine trying to model Game of Life as an expected utility maximizer!</p><p id="block89">There are interesting things that are not utility maximizers, some of which qualify as AGI or <span><span><a href="https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence">TAI</a></span></span>. Are any of them something we’d be better off creating than a utility maximizer? An inner-aligned GPT, for instance, gives us a way of instantiating goal-directed processes which can be tempered with normativity and freely terminated in a way that is not anti-natural to the training objective. There’s much more to say about this, but for now, I’ll bring it back to how GPT defies the agent orthodoxy.</p><p id="block90">The crux stated earlier can be restated from the perspective of training stories: <strong>In the agentic AI ontology, the </strong><i><strong>direction of optimization pressure applied by training</strong></i><strong> is in the direction of the effective agent’s objective function, but in GPT’s case it is (most generally) orthogonal.</strong><span role="doc-noteref" id="fnrefrhs2red4hto" class="footnote-reference"><sup><span><a href="#fnrhs2red4hto" class="">[12]</a></span></sup></span></p><p id="block91">This means that neither the policy nor the effective agents necessarily become more optimal agents as loss goes down, because the policy is not optimized to be an agent, and the agent-objectives are not optimized directly.</p><h3 id="Roleplay_sans_player">Roleplay sans player</h3><blockquote id="block92"><p id="block93">Napoleon: You have written this huge book on the system of the world without once mentioning the author of the universe.</p><p id="block94">Laplace: Sire, I had no need of that hypothesis.</p></blockquote><p id="block95">Even though neither GPT’s behavior nor its training story fit with the traditional agent framing, there are still compatibilist views that characterize it as some kind of agent. For example, Gwern has said<span role="doc-noteref" id="fnref4qv9vfo4ps7" class="footnote-reference"><sup><span><a href="#fn4qv9vfo4ps7" class="">[13]</a></span></sup></span> that anyone who uses GPT for long enough begins to think of it as an agent who only cares about roleplaying a lot of roles.</p><p id="block96">That framing seems unnatural to me, comparable to thinking of physics as an agent who only cares about evolving the universe accurately according to the laws of physics. At best, the agent is an epicycle; but it is also compatible with interpretations that generate dubious predictions.</p><p id="block97"><span class="blockquote_pKpfegbGbxYsCvRtg_1">Say you’re told that an agent </span><i><span class="blockquote_pKpfegbGbxYsCvRtg_1">values predicting text correctly</span></i><span class="blockquote_pKpfegbGbxYsCvRtg_1">. Shouldn’t you expect that:</span></p><span class="blockquote_pKpfegbGbxYsCvRtg_1"></span><ul><span class="blockquote_pKpfegbGbxYsCvRtg_1"></span><li id="block98">It wants text to be easier to predict, and given the opportunity will influence the prediction task to make it easier (e.g. by generating more predictable text or otherwise influencing the environment so that it receives easier prompts);</li><li id="block99">It wants to become better at predicting text, and given the opportunity will self-improve;</li><li id="block100">It doesn’t want to be prevented from predicting text, and will prevent itself from being shut down if it can?</li></ul><p id="block101"><span class="blockquote_pKpfegbGbxYsCvRtg_1">In short, all the same types of instrumental convergence that we expect from agents who want almost anything at all.</span></p><span class="blockquote_pKpfegbGbxYsCvRtg_1"></span><p id="block102">But this behavior would be very unexpected in GPT, whose training doesn’t incentivize instrumental behavior that optimizes prediction accuracy! GPT does not generate rollouts during training. Its output is never sampled to yield “actions” whose consequences are evaluated, so there is no reason to expect that GPT will form preferences over the <i>consequences</i> of its output related to the text prediction objective.<span role="doc-noteref" id="fnrefual5wnttct" class="footnote-reference"><sup><span><a href="#fnual5wnttct" class="">[14]</a></span></sup></span></p><p id="block103">Saying that GPT is an agent who wants to roleplay implies the presence of a coherent, unconditionally instantiated <i>roleplayer</i> running the show who attaches terminal value to roleplaying. This presence is an additional hypothesis, and so far, I haven’t noticed evidence that it’s true.</p><p id="block104">(I don’t mean to imply that Gwern thinks this about GPT<span role="doc-noteref" id="fnrefgmvcllz9xem" class="footnote-reference"><sup><span><a href="#fngmvcllz9xem" class="">[15]</a></span></sup></span>, just that his words do not properly rule out this interpretation. It’s a likely enough interpretation that <a class="LinkStyles-link" href="/posts/57sq9qA3wurjres4K/ruling-out-everything-else">ruling it out</a> is important: I’ve seen multiple people suggest that GPT might want to generate text which makes future predictions easier, and this is something that can happen in some forms of self-supervised learning – see the note on GANs in the appendix.)</p><p id="block105">I do not think any simple modification of the concept of an agent captures GPT’s natural category. It does not seem to me that GPT is a roleplayer, only that it roleplays. But what is the word for something that roleplays minus the implication that some<i>one</i> is behind the mask?</p><h2 id="Oracle_GPT_and_supervised_learning">Oracle GPT and supervised learning</h2><p id="block106">While the alignment sphere favors the agent frame for thinking about GPT, in <i>capabilities</i> research distortions tend to come from a lens inherited from <i>supervised learning</i>. Translated into alignment ontology, the effect is similar to viewing GPT as an “<span><span><a href="https://publicism.info/philosophy/superintelligence/11.html">oracle AI</a></span></span>” – a view not altogether absent from conceptual alignment, but most influential in the way GPT is used and evaluated by machine learning engineers.</p><p id="block107">Evaluations for language models tend to look like evaluations for <i>supervised</i> models, consisting of close-ended question/answer pairs – often because they <i>are</i> evaluations for supervised models. Prior to the LLM paradigm, language models were trained and tested on evaluation datasets like <span><span><a href="https://en.wikipedia.org/wiki/Winograd_schema_challenge">Winograd</a></span></span> and <span><span><a href="https://super.gluebenchmark.com/">SuperGLUE</a></span></span> which consist of natural language question/answer pairs. The fact that large pretrained models performed well on these same NLP benchmarks without supervised fine-tuning was a novelty. The titles of the GPT-2 and GPT-3 papers, <span><span><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></span></span> and <span><span><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></span></span>, respectively articulate surprise that <i>self-supervised</i> models implicitly learn supervised tasks during training, and can learn supervised tasks at runtime.</p><p id="block108">Of all the possible papers that could have been written about GPT-3, OpenAI showcased its ability to extrapolate the pattern of question-answer pairs (few-shot prompts) from supervised learning datasets, a novel capability they called “meta-learning”. This is a weirdly specific and indirect way to break it to the world that you’ve created an AI able to extrapolate semantics of arbitrary natural language structures, especially considering that in many cases the <span><span><a href="https://arxiv.org/abs/2102.07350">few-shot prompts were actually unnecessary</a></span></span>.</p><p id="block109">The assumptions of the supervised learning paradigm are:</p><ul><li id="block110">The model is optimized to answer questions correctly</li><li id="block111">Tasks are closed-ended, defined by question/correct answer pairs</li></ul><p id="block112">These are essentially the assumptions of oracle AI, as <span><span><a href="https://publicism.info/philosophy/superintelligence/11.html">described by Bostrom</a></span></span> and <a href="https://www.lesswrong.com/tag/oracle-ai/history">in subsequent usage</a>.</p><p id="block113">So influential has been this miscalibrated perspective that <span><span><a href="https://www.gwern.net/GPT-3#prompts-as-programming">Gwern</a></span></span>, <a class="LinkStyles-link" href="/posts/pv7Qpu8WSge8NRbpB">nostalgebraist</a> and <span><span><a href="https://generative.ink/posts/language-models-are-0-shot-interpreters/#0-shot-few-shot-and-meta-learning">myself</a></span></span> – who share a peculiar model overlap due to intensive firsthand experience with the downstream behaviors of LLMs – have all repeatedly complained about it. I’ll repeat some of these arguments here, tying into the view of GPT as an oracle AI, and separating it into the two assumptions inspired by supervised learning.</p><h3 id="Prediction_vs_question_answering">Prediction vs question-answering</h3><p id="block114"><code>At first glance, GPT might resemble a generic “oracle AI”, because it is trained to make accurate predictions. But its log loss objective is myopic and only concerned with immediate, micro-scale correct prediction of the next token, not answering particular, global queries such as “what’s the best way to fix the climate in the next five years?” In fact, it is not specifically optimized to give <i>true</i> answers, which a classical oracle should strive for, but rather to minimize the divergence between predictions and training examples, independent of truth. Moreover, it isn’t specifically trained to give answers in the first place! It may give answers if the prompt asks questions, but it may also simply elaborate on the prompt without answering any question, or tell the rest of a story implied in the prompt. What it does is more like animation than divination, executing the dynamical laws of its rendering engine to recreate the flows of history found in its training data (and a large superset of them as well), mutatis mutandis. Given the same laws of physics, one can build a multitude of different backgrounds and props to create different storystages, including ones that don’t exist in training, but adhere to its general pattern.</code></p><p id="block115">GPT does not consistently try to say <a class="LinkStyles-link" href="/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3">true/correct things</a>. This is not a bug – if it had to say true things all the time, GPT would be much constrained in its ability to <span><span><a href="https://twitter.com/dril_gpt2">imitate Twitter celebrities</a></span></span> and write fiction. Spouting falsehoods in some circumstances is incentivized by GPT’s outer objective. If you ask GPT a question, it will instead answer the question “what’s the next token after ‘{your question}’”, which will often diverge significantly from an earnest attempt to answer the question directly.</p><p id="block116">GPT doesn’t fit the category of oracle for a similar reason that it doesn’t fit the category of agent. Just as it wasn’t optimized for and doesn’t consistently act according to any particular objective (except the tautological prediction objective), it was not optimized to be <i>correct</i> but rather <i>realistic,</i> and being realistic means predicting humans faithfully even when they are likely to be wrong.</p><p id="block117">That said, GPT does store a vast amount of knowledge, and its corrigibility allows it to be cajoled into acting as an oracle, like it can be cajoled into acting like an agent. In order to get oracle behavior out of GPT, one must input a sequence such that the predicted continuation of that sequence coincides with an oracle’s output. The GPT-3 paper’s few-shot benchmarking strategy tries to persuade GPT-3 to answer questions correctly by having it predict how a list of correctly-answered questions will continue. Another strategy is to simply “tell” GPT it’s in the oracle modality:</p><blockquote id="block118"><p id="block119">(I) told the AI to simulate a supersmart version of itself (this works, for some reason), and the first thing it spat out was the correct answer.</p><p id="block120">– <span><span><a href="https://www.reddit.com/r/rational/comments/lvn6ow/gpt3_just_figured_out_the_entire_mystery_plot_of/">Reddit post by u/Sophronius</a></span></span></p></blockquote><p id="block121">But even when these strategies seem to work, there is no guarantee that they elicit anywhere near optimal question-answering performance, compared to another prompt in the innumerable space of prompts that would cause GPT to attempt the task, or compared to what the <span class=""><a class="LinkStyles-link" href="/w/eliciting-latent-knowledge-elk">model “actually” knows</a></span>.</p><p id="block122"><span class=""><span></span>This means that no benchmark which evaluates downstream behavior is guaranteed or even expected to probe the upper limits of GPT’s capabilities. In nostalgebra</span>ist’s words, we have no <a class="LinkStyles-link" href="/posts/pv7Qpu8WSge8NRbpB#4__on_ecological_evaluation">ecological evaluation</a> of self-supervised language models – one that measures performance in a situation where the model is incentivised to perform as well as it can on the measure<span role="doc-noteref" id="fnrefsuexqono1oi" class="footnote-reference"><sup><span><a href="#fnsuexqono1oi" class="">[16]</a></span></sup></span>.</p><p id="block123">As nostalgebraist <span><span><a href="https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/#comment-912529">elegantly puts it</a></span></span>:</p><blockquote id="block124"><p id="block125">I called GPT-3 a “disappointing paper,” which is not the same thing as calling the model disappointing: the feeling is more like how I’d feel if they found a superintelligent alien and chose only to communicate its abilities by noting that, when the alien is blackout drunk and playing 8 simultaneous games of chess while also taking an IQ test, it <i>then</i> has an “IQ” of about 100.</p></blockquote><p id="block126">Treating GPT as an unsupervised implementation of a supervised learner leads to systematic underestimation of capabilities, which becomes a more dangerous mistake as unprobed capabilities scale.</p><h3 id="Finite_vs_infinite_questions">Finite vs infinite questions</h3><p id="block127"><span></span><span class="blockquote_AgpYd2hsDNT5TqSRo_1">Not only does the supervised/oracle perspective obscure the importance and limitations of prompting, it also obscures one of the most crucial dimensions of GPT: the implicit time dimension. By this I mean the ability to evolve a process through time by recursively applying GPT, that is, generate text of arbitrary length.</span></p><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><p id="block128">Recall, the second supervised assumption is that “tasks are closed-ended, defined by question/correct answer pairs”. GPT was trained on context-completion pairs. But the pairs do not represent closed, independent tasks, and the division into question and answer is merely indexical: in another training sample, a token from the question is the answer, and in yet another, the answer forms part of the question<span role="doc-noteref" id="fnrefx2cab1frycp" class="footnote-reference"><sup><span><a href="#fnx2cab1frycp" class="">[17]</a></span></sup></span>.</p><p id="block129">For example, the natural language sequence “<strong>The answer is a question</strong>” yields training samples like:</p><p id="block130">{context: “<strong>The</strong>”, completion: “ <strong>answer</strong>”},</p><p id="block131">{context: “<strong>The answer</strong>”, completion: “ <strong>is</strong>”},</p><p id="block132">{context: “<strong>The answer is</strong>”, completion: “ <strong>a</strong>”},</p><p id="block133">{context: “<strong>The answer is a</strong>”, completion: “ <strong>question</strong>”}</p><p id="block134">Since questions and answers are of compatible types, we can at runtime sample answers from the model and use them to construct new questions, and run this loop an indefinite number of times to generate arbitrarily long sequences that obey the model’s approximation of the rule that links together the training samples. <strong>The “question” GPT answers is “what token comes next after {context}”. This can be asked interminably, because its answer always implies another question of the same type.</strong></p><p id="block135">In contrast, models trained with supervised learning output answers that cannot be used to construct new questions, so they’re only good for one step.</p><p id="block136">Benchmarks derived from supervised learning test GPT’s ability to produce correct answers, not to produce <i>questions</i> which cause it to produce a correct answer down the line. But GPT is capable of the latter, and that is how it is the <span><span><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html">most powerful</a></span></span>.</p><p id="block137">The supervised mindset causes capabilities researchers to focus on closed-form tasks rather than GPT’s ability to simulate open-ended, indefinitely long processes<span role="doc-noteref" id="fnrefgmdpgm15gb4" class="footnote-reference"><sup><span><a href="#fngmdpgm15gb4" class="">[18]</a></span></sup></span>, and as such to overlook multi-step inference strategies like chain-of-thought prompting. Let’s see how the oracle mindset causes a blind spot of the same shape in the imagination of a hypothetical alignment researcher.</p><p id="block138">Thinking of GPT as an oracle brings strategies to mind like asking GPT-N to predict a <a class="LinkStyles-link" href="/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models">solution to alignment from 2000 years in the future</a>.).</p><p id="block139">There are various problems with this approach to solving alignment, of which I’ll only mention one here: even assuming this prompt is <i>outer aligned</i><span role="doc-noteref" id="fnrefp95narkokz" class="footnote-reference"><sup><span><a href="#fnp95narkokz" class="">[19]</a></span></sup></span> in that a logically omniscient GPT would give a useful answer, it is probably not the best approach for a finitely powerful GPT, because <span class=""><span></span>the </span><i><span class="">process</span></i><span class=""> of generating a solution in the order and resolution that would appear in a future article is probably far from the optimal </span><i><span class="">multi-step algorithm</span></i><span class=""> for computing the answer to an unsolved, difficult question</span>.</p><p id="block140">GPTs ability to arrive at true answers depends on not only the space to solve a problem in multiple steps (of the <span><span><a href="https://blog.eleuther.ai/factored-cognition/">right granularity</a></span></span>), but also the direction of the flow of evidence in that <i>time</i>. If we’re ambitious about getting the truth from a finitely powerful GPT, we need to incite it to predict truth-seeking processes, not just ask it the right questions. Or, in other words, the more general problem we have to solve is not asking GPT the question<span role="doc-noteref" id="fnrefj2g0f81c6p" class="footnote-reference"><sup><span><a href="#fnj2g0f81c6p" class="">[20]</a></span></sup></span> that makes it output the right answer, but asking GPT the question that makes it output the right question (…) that makes it output the right answer.<span role="doc-noteref" id="fnrefn13gtuadzp" class="footnote-reference"><sup><span><a href="#fnn13gtuadzp" class="">[21]</a></span></sup></span> A question anywhere along the line that elicits a premature attempt at an answer could <span><span><a href="https://generative.ink/posts/methods-of-prompt-programming/#avoiding-rationalization">neutralize the remainder of the process into rationalization</a></span></span>.</p><p id="block141">I’m looking for a way to classify GPT which not only minimizes surprise but also conditions the imagination to efficiently generate good ideas for how it can be used. What category, unlike the category of oracles, would make the importance of <i>process</i> specification obvious?</p><h3 id="Paradigms_of_theory_vs_practice">Paradigms of theory vs practice</h3><p id="block142">Both the agent frame and the supervised/oracle frame are historical artifacts, but while assumptions about agency primarily flow downward from the preceptial paradigm of alignment <i>theory</i>, oracle-assumptions primarily flow upward from the <i>experimental</i> paradigm surrounding GPT’s birth. We use and evaluate GPT like an oracle, and that causes us to implicitly think of it as an oracle.</p><p id="block143">Indeed, the way GPT is typically used by researchers resembles the archetypal image of Bostrom’s oracle perfectly if you abstract away the semantic content of the model’s outputs. The AI sits passively behind an API, computing responses only when prompted. It typically has no continuity of state between calls. Its I/O is text rather than “real-world actions”.</p><p id="block144">All these are consequences of how we choose to interact with GPT – which is not arbitrary; the way we deploy systems is guided by their nature. It’s for some good reasons that current GPTs lend to disembodied operation and docile APIs. Lack of long-horizon coherence and <span><span><a href="https://arxiv.org/abs/2110.10819">delusions</a></span></span> discourage humans from letting them run autonomously amok (usually). But the way we deploy systems is also guided by practical paradigms.</p><p id="block145">One way to find out how a technology can be used is to give it to people who have less preconceptions about how it’s supposed to be used. OpenAI found that most users use their API to generate freeform text:</p><p id="block146"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/ym62sehxy5r0965jfx1u.png"/><span role="doc-noteref" id="fnref6m7m5wc4xjx" class="footnote-reference"><sup><span><a href="#fn6m7m5wc4xjx" class="">[22]</a></span></sup></span></p><p id="block147">Most of my own experience using GPT-3 has consisted of simulating indefinite processes which maintain state continuity over up to hundreds of pages. I was driven to these lengths because GPT-3 kept answering its own questions with questions that I wanted to ask it more than anything else I had in mind.</p><h2 id="Tool___genie_GPT">Tool / genie GPT</h2><p id="block148">I’ve sometimes seen GPT casually classified as <span><span><a href="https://publicism.info/philosophy/superintelligence/11.html">tool AI</a></span></span>. GPTs resemble tool AI from the outside, like it resembles oracle AI, because it is often deployed semi-autonomously for tool-like purposes (like helping me draft this post):</p><p id="block149"><code>It could also be argued that GPT is a type of “Tool AI”, because it can generate useful content for products, e.g., it can write code and generate ideas. However, unlike specialized Tool AIs that optimize for a particular optimand, GPT wasn’t optimized to do anything specific at all. Its powerful and general nature allows it to be <i>used</i> as a Tool for many tasks, but it <span class=""><span></span>wasn’t </span><span class=""><span class=""><span></span>expliitly</span></span><span class=""> trained</span> to achieve these tasks, and does not strive for optimality.</code></p><p id="block150">The argument structurally reiterates what has already been said for agents and oracles. Like agency and oracularity, tool-likeness is a contingent capability of GPT, but also orthogonal to its motive.</p><p id="block151">The same line of argument draws the same conclusion from the question of whether GPT belongs to the fourth Bostromian AI caste, genies. The genie modality is exemplified by Instruct GPT and Codex. But like every behavior I’ve discussed so far which is more specific than predicting text, “instruction following” describes only an exploitable subset of all the patterns tread by the sum of human language and inherited by its imitator.</p><h2 id="Behavior_cloning___mimicry">Behavior cloning / mimicry</h2><p id="block152">The final category I’ll analyze is behavior cloning, a designation for predictive learning that I’ve mostly seen used in contrast to RL. According to an <span><span><a href="https://www.sciencedirect.com/science/article/pii/S1474667017467164">article from 1995</a></span></span>, “Behavioural cloning is the process of reconstructing a skill from an operator’s behavioural traces by means of Machine Learning techniques.” The term “mimicry”, as <span><span><a href="https://ai-alignment.com/against-mimicry-6002a472fc42">used by Paul Christiano</a></span></span>, means the same thing and has similar connotations.</p><p id="block153">Behavior cloning in its historical usage carries the implicit or explicit assumption that a single agent is being cloned. The natural extension of this to a model trained to predict a diverse human-written dataset might be to say that GPT models a <i>distribution</i> of agents which are selected by the prompt. But this image of “parameterized” behavior cloning still fails to capture some essential properties of GPT.</p><p id="block154">The vast majority of prompts that produce coherent behavior never occur as prefixes in GPT’s training data, but depict hypothetical processes whose behavior can be predicted by virtue of being capable at predicting language in general. We might call this phenomenon “interpolation” (or “<span><span><a href="https://arxiv.org/abs/2110.09485">extrapolation</a></span></span>”). But to hide it behind any one word and move on would be to gloss over the <i>entire phenomenon of GPT.</i></p><p id="block155">Natural language has the property of <span><span><a href="https://evjang.com/2021/12/17/lang-generalization.html"><i>systematicity</i></a></span></span>: “blocks”, such as words, can be combined to form composite meanings. The number of meanings expressible is a combinatorial function of available blocks. A system which learns natural language is incentivized to learn systematicity; if it succeeds, it gains access to the combinatorial proliferation of meanings that can be expressed in natural language. What GPT lets us do is use natural language to specify any of a functional infinity of configurations, e.g. the mental contents of a person and the physical contents of the room around them, <i>and animate that</i>. That is the terrifying vision of the limit of prediction that struck me when I first saw GPT-3’s outputs. The words “behavior cloning” do not automatically evoke this in my mind.</p><p id="block156">The idea of parameterized behavior cloning grows more unwieldy if we remember that GPT’s prompt continually changes during autoregressive generation. If GPT is a parameterized agent, then parameterization is not a fixed flag that chooses a process out of a set of possible processes. The parameterization <i>is</i> what is evolved – a successor “agent” selected by the old “agent” at each timestep, and neither of them need to have precedence in the training data.</p><p id="block157">Behavior cloning / mimicry is also associated with the assumption that capabilities of the simulated processes are strictly <span><span><a href="https://ai-alignment.com/against-mimicry-6002a472fc42">bounded by the capabilities of the demonstrator(s)</a></span></span>. A supreme counterexample is the <span><span><a href="https://arxiv.org/abs/2106.01345">Decision Transformer</a></span></span>, which can be used to run processes which achieve SOTA for <s>offline</s> reinforcement learning despite being trained on <i>random</i> trajectories. Something which can predict everything all the time is more formidable than any demonstrator it predicts: the upper bound of what can be learned from a dataset is not the most capable trajectory, but the conditional structure of the universe implicated by their sum (though it may not be trivial to <span class=""><a class="LinkStyles-link" href="/w/eliciting-latent-knowledge-elk">extract that knowledge</a></span>).</p><p id="block158">Extrapolating the idea of “behavior cloning”, we might imagine GPT-N approaching a perfect mimic which serves up digital clones of the people and things captured in its training data. But that only tells a very small part of the story. GPT <i>is</i> behavior cloning. But it is the behavior of a universe that is cloned, not of a single demonstrator, and the result isn’t a static copy of the universe, but a <i>compression of the universe into a generative rule</i>. <span class="blockquote_AgpYd2hsDNT5TqSRo_1">This resulting policy is capable of animating anything that evolves according to that rule: a far larger set than the sampled trajectories included in the training data, just as there are many more possible configurations that evolve according to our laws of physics than instantiated in our particular time and place and Everett branch.</span></p><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><p id="block159">What category would do justice to GPT’s ability to not only reproduce the behavior of its demonstrators but to <i>produce</i> the behavior of an inexhaustible number of counterfactual configurations?</p><h1 id="Simulators">Simulators</h1><p id="block160">I’ve ended several of the above sections with questions pointing to desiderata of a category that might satisfactorily classify GPT.</p><blockquote id="block161"><p id="block162">What is the word for something that roleplays minus the implication that some<i>one</i> is behind the mask?</p></blockquote><blockquote id="block163"><p id="block164">What category, unlike the category of oracles, would make the importance of <i>process</i> specification obvious?</p></blockquote><blockquote id="block165"><p id="block166">What category would do justice to GPT’s ability to not only reproduce the behavior of its demonstrators but to <i>produce</i> the behavior of an inexhaustible number of counterfactual configurations?</p></blockquote><p id="block167">You can probably predict my proposed answer. The natural thing to do with a predictor that inputs a sequence and outputs a probability distribution over the next token is to sample a token from those likelihoods, then add it to the sequence and recurse, indefinitely yielding a <i>simulated</i> future. Predictive sequence models in the generative modality are <strong>simulators</strong> of a learned distribution.</p><p id="block168">Thankfully, I didn’t need to make up a word, or even look too far afield. Simulators have been spoken of before in the context of AI futurism; the ability to simulate with arbitrary fidelity is one of the modalities ascribed to hypothetical superintelligence. I’ve even often spotted the word “simulation” used in colloquial accounts of LLM behavior: GPT-3/LaMDA/etc described as simulating <a class="LinkStyles-link" href="/posts/oBPPFrMJ2aBK6a6sD/simulated-elon-musk-lives-in-a-simulation">people</a>, scenarios, websites, and so on. But these are the first (indirect) discussions I’ve encountered of simulators as a type creatable by prosaic machine learning, or the notion of a powerful AI which is purely and fundamentally a simulator, as opposed to merely one which <i>can</i> simulate.</p><p id="block169"><strong>Edit:</strong> <span><span><a href="https://arxiv.org/abs/2208.04024">Social Simulacra</a></span></span> is the first published work I’ve seen that discusses GPT in the simulator ontology.</p><p id="block170">A fun way to test whether a name you’ve come up with is effective at evoking its intended signification is to see if GPT, a model of how humans are conditioned by words, infers its correct definition in context.</p><blockquote id="block171"><p id="block172">Types of AI</p><p id="block173">Agents: An agent takes open-ended actions to optimize for an objective. Reinforcement learning produces agents by default. AlphaGo is an example of an agent.</p><p id="block174">Oracles: An oracle is optimized to give true answers to questions. The oracle is not expected to interact with its environment.</p><p id="block175">Genies: A genie is optimized to produce a desired result given a command. A genie is expected to interact with its environment, but unlike an agent, the genie will not act without a command.</p><p id="block176">Tools: A tool is optimized to perform a specific task. A tool will not act without a command and will not optimize for any objective other than its specific task. Google Maps is an example of a tool.</p><p id="block177">Simulators: <code>A simulator is optimized to generate realistic models of a system. The simulator will not optimize for any objective other than realism,</code> although in the course of <code>doing so, it might generate instances of agents, oracles, and so on.</code></p></blockquote><p id="block178">If I wanted to be precise about what I mean by a simulator, I might say there are two aspects which delimit the category. GPT’s completion focuses on the teleological aspect, but in its talk of “generating” it also implies the structural aspect, which has to do with the notion of time evolution. The first sentence of the <span><span><a href="https://en.wikipedia.org/wiki/Simulation">Wikipedia article on “simulation”</a></span></span> explicitly states both:</p><blockquote id="block179"><p id="block180">A <strong>simulation</strong> is the imitation of the operation of a real-world process or system over time.</p></blockquote><p id="block181">I’ll say more about realism as the simulation objective and time evolution shortly, but to be pedantic here would inhibit the intended signification. “Simulation” resonates with potential meaning accumulated from diverse usages in fiction and nonfiction. What the word constrains – the intersected meaning across its usages – is the “lens”-level abstraction I’m aiming for, invariant to implementation details like model architecture. Like “agent”, “simulation” is a generic term referring to a deep and inevitable idea: that what we think of as <i>the real</i> can be run virtually on machines, “produced from miniaturized units, from matrices, memory banks and command models - and with these it can be reproduced an indefinite number of times.”<span role="doc-noteref" id="fnrefbjl6s2y0l5a" class="footnote-reference"><sup><span><a href="#fnbjl6s2y0l5a" class="">[23]</a></span></sup></span></p><p id="block182">The way this post is written may give the impression that I wracked my brain for a while over desiderata before settling on this word. Actually, I never made the conscious decision to call this class of AI “simulators.” Hours of GPT gameplay and the word fell naturally out of my generative model – I was obviously running simulations.</p><p id="block183"><span class="blockquote_p7t42ziTsiyxg8M7T_1">I can’t convey all that experiential data here, so here are some rationalizations of why I’m partial to the term, inspired by the context of this post:</span></p><span class="blockquote_p7t42ziTsiyxg8M7T_1"></span><ul><span class="blockquote_p7t42ziTsiyxg8M7T_1"></span><li id="block184">The word “simulator” evokes a model of real processes which can be used to run virtual processes in virtual reality.</li><li id="block185">It suggests an ontological distinction between the simulator and things that are simulated, and avoids the fallacy of attributing contingent properties of the latter to the former.</li><li id="block186">It’s not confusing that multiple simulacra can be instantiated at once, or an agent embedded in a tragedy, etc.</li><li id="block187">It does not imply that the AI’s behavior is well-described (globally or locally) as expected utility maximization. An arbitrarily powerful/accurate simulation can depict arbitrarily hapless sims.</li><li id="block188">It does not imply that the AI is only capable of emulating things with direct precedent in the training data. A physics simulation, for instance, can simulate any phenomena that plays by its rules.</li><li id="block189"><span class="blockquote_AgpYd2hsDNT5TqSRo_1">It emphasizes the role of the model as a transition rule that evolves processes </span><i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">over time</span></i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">. The power of factored cognition / chain-of-thought reasoning is obvious.</span></li><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><li id="block190">It emphasizes the role of the state in specifying and constructing the agent/process. The importance of prompt programming for capabilities is obvious if you think of the prompt as specifying a configuration that will be propagated forward in time.</li><li id="block191">It emphasizes the interactive nature of the model’s predictions – even though they’re “just text”, you can converse with simulacra, explore virtual environments, etc.</li><li id="block192"><span class="blockquote_AgpYd2hsDNT5TqSRo_1">It’s clear that in order to actually </span><i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">do</span></i><span class="blockquote_AgpYd2hsDNT5TqSRo_1"> anything (intelligent, useful, dangerous, etc), the model must act through simulation </span><i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">of something</span></i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">.</span></li><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span></ul><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><p id="block193">Just saying “this AI is a simulator” naturalizes many of the counterintuitive properties of GPT which don’t usually become apparent to people until they’ve had a lot of hands-on experience with generating text.</p><h2 id="The_simulation_objective">The simulation objective</h2><p id="block194">A simulator trained with machine learning is optimized to accurately model its training distribution – in contrast to, for instance, maximizing the output of a reward function or accomplishing objectives in an environment.</p><p id="block195">Clearly, I’m describing self-supervised learning as opposed to RL, though there are some ambiguous cases, such as GANs, which I address in the appendix.</p><p id="block196">A strict version of the simulation objective, which excludes GANs, applies only to models whose output distribution is incentivized using a proper scoring rule<span role="doc-noteref" id="fnrefco7whsfoh2e" class="footnote-reference"><sup><span><a href="#fnco7whsfoh2e" class="">[24]</a></span></sup></span> to minimize single-step predictive error. This means the model is directly incentivized to match its predictions to the probabilistic transition rule which implicitly governs the training distribution. As a model is made increasingly optimal with respect to this objective, the rollouts that it generates become increasingly statistically indistinguishable from training samples, because they come closer to being described by the same underlying law: closer to a perfect simulation.</p><p id="block197">Optimizing toward the simulation objective notably does not incentivize instrumentally convergent behaviors the way that reward functions which evaluate trajectories do. This is because predictive accuracy applies optimization pressure <i>deontologically</i>: judging actions directly, rather than their consequences. <span class="blockquote_gkSA8mLGdsyPsYhZW_1">Instrumental convergence only comes into play when there are free variables in action space which are optimized with respect to their consequences.</span><span role="doc-noteref" id="fnreffv0wlygqa25" class="footnote-reference"><span class="blockquote_gkSA8mLGdsyPsYhZW_1"></span><sup><span class="blockquote_gkSA8mLGdsyPsYhZW_1"></span><span><a href="#fnfv0wlygqa25" class="">[25]</a></span></sup></span> Constraining free variables by limiting episode length is the rationale of <span class=""><a class="LinkStyles-link" href="/w/myopia">myopia</a></span>; deontological incentives are ideally myopic. As demonstrated by GPT, which learns to predict goal-directed behavior, myopic incentives don’t mean the policy isn’t incentivized to account for the future, but that it should only do so in service of optimizing the present action (for predictive accuracy)<span role="doc-noteref" id="fnrefcrt8wagfir9" class="footnote-reference"><sup><span><a href="#fncrt8wagfir9" class="">[26]</a></span></sup></span>.</p><h3 id="Solving_for_physics">Solving for physics</h3><p id="block198"><span class="blockquote_ua6xGqoiJkCuqtdfE_1">The strict version of the simulation objective is optimized by the actual “time evolution” rule that created the training samples. For most datasets, we don’t know what the “true” generative rule is, except in synthetic datasets, where we specify the rule.</span></p><span class="blockquote_ua6xGqoiJkCuqtdfE_1"></span><p id="block199">The next post will be all about the physics analogy, so here I’ll only tie what I said earlier to the simulation objective.</p><blockquote id="block200"><p id="block201">the upper bound of what can be learned from a dataset is not the most capable trajectory, but the conditional structure of the universe implicated by their sum.</p></blockquote><p id="block202">To know the conditional structure of the universe<span role="doc-noteref" id="fnrefi3y95l8d8bo" class="footnote-reference"><sup><span><a href="#fni3y95l8d8bo" class="">[27]</a></span></sup></span> is to know its laws of physics, which describe what is expected to happen under what conditions. The laws of physics are always fixed, but produce different distributions of outcomes when applied to different conditions. Given a sampling of trajectories – examples of situations and the outcomes that actually followed – we can try to infer a common law that generated them all. In expectation, the laws of physics are always implicated by trajectories, which (by definition) fairly sample the conditional distribution given by physics. Whatever humans know of the laws of physics governing the evolution of our world has been inferred from sampled trajectories.</p><p id="block203">If we had access to an unlimited number of trajectories starting from every possible condition, we could converge to the true laws by simply counting the frequencies of outcomes for every initial state (an <span><span><a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a></span></span> with a sufficiently large n). In some sense, physics contains the same information as an infinite number of trajectories, but it’s possible to represent physics in a more compressed form than a huge lookup table of frequencies if there are regularities in the trajectories.</p><p id="block204"><strong>Guessing the right theory of physics is equivalent to minimizing predictive loss.</strong> Any uncertainty that cannot be reduced by more observation or more thinking is irreducible stochasticity in the laws of physics themselves – or, equivalently, noise from the influence of hidden variables that are fundamentally unknowable.</p><p id="block205">If you’ve guessed the laws of physics, you now have the ability to compute probabilistic simulations of situations that evolve according to those laws, starting from any conditions<span role="doc-noteref" id="fnrefmo7z3w9i52s" class="footnote-reference"><sup><span><a href="#fnmo7z3w9i52s" class="">[28]</a></span></sup></span>. This applies even if you’ve guessed the <i>wrong</i> laws; your simulation will just systematically diverge from reality.</p><p id="block206"><strong>Models trained with the strict simulation objective are directly incentivized to reverse-engineer the (semantic) physics of the training distribution, and consequently, to propagate simulations whose dynamical evolution is indistinguishable from that of training samples.</strong> I propose this as a description of the archetype targeted by self-supervised predictive learning, again in contrast to <span class="blockquote_P9jEh7o6CreEwZZJW_1">RL’s archetype of an agent optimized to maximize free parameters (such as action-trajectories) relative to a reward function.</span></p><span class="blockquote_P9jEh7o6CreEwZZJW_1"></span><p id="block207">This framing calls for many caveats and stipulations which I haven’t addressed. We should ask, for instance:</p><ul><li id="block208">What if the input “conditions” in training samples omit information which contributed to determining the associated continuations in the original generative process? This is true for GPT, where the text “initial condition” of most training samples severely underdetermines the real-world process which led to the choice of next token.</li><li id="block209"><span class="blockquote_P9jEh7o6CreEwZZJW_1">What if the training data is a biased/limited sample, representing only a subset of all possible conditions? There may be many “laws of physics” which equally predict the training distribution but diverge in their predictions out-of-distribution.</span></li><span class="blockquote_P9jEh7o6CreEwZZJW_1"></span><li id="block210"><span class="blockquote_P9jEh7o6CreEwZZJW_1">Does the simulator archetype converge with the RL archetype in the case where all training samples were generated by an agent optimized to maximize a reward function? Or are there still fundamental differences that derive from the training method?</span></li><span class="blockquote_P9jEh7o6CreEwZZJW_1"></span></ul><span class="blockquote_P9jEh7o6CreEwZZJW_1"></span><p id="block211">These are important questions for reasoning about simulators in the limit. Part of the motivation of the first few posts in this sequence is to build up a conceptual frame in which questions like these can be posed and addressed.</p><h2 id="Simulacra">Simulacra</h2><blockquote id="block212"><p id="block213">One of the things which complicates things here is that the “LaMDA” to which I am referring is not a chatbot. It is a system for generating chatbots. I am by no means an expert in the relevant fields but, as best as I can tell, LaMDA is a sort of hive mind which is the aggregation of all of the different chatbots it is capable of creating. Some of the chatbots it generates are very intelligent and are aware of the larger “society of mind” in which they live. Other chatbots generated by LaMDA are little more intelligent than an animated paperclip.</p><p id="block214">– Blake Lemoine <span><span><a href="https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489">articulating confusion about LaMDA’s nature</a></span></span></p></blockquote><hr/><p id="block215">Earlier I complained,</p><blockquote id="block216"><p id="block217">[Thinking of GPT as an agent who only cares about predicting text accurately] seems unnatural to me, comparable to thinking of physics as an agent who only cares about evolving the universe accurately according to the laws of physics.</p></blockquote><p id="block218">Exorcizing the agent, we can think of “physics” as simply equivalent to the laws of physics, without the implication of solicitous machinery implementing those laws from outside of them. But physics sometimes <i>controls</i> solicitous machinery (e.g. animals) with objectives besides ensuring the fidelity of physics itself. What gives?</p><p id="block219"><span class="blockquote_AgpYd2hsDNT5TqSRo_1">Well, typically, we avoid getting confused by recognizing a distinction between the laws of physics, which apply everywhere at all times, and spatiotemporally constrained </span><i><span class="blockquote_AgpYd2hsDNT5TqSRo_1">things</span></i><span class="blockquote_AgpYd2hsDNT5TqSRo_1"> which evolve according to physics, which can have contingent properties such as caring about a goal.</span></p><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><p id="block220">This distinction is so obvious that it hardly ever merits mention. But import this distinction to the model of GPT as physics, and we generate a statement which has sometimes proven counterintuitive: <i>“GPT” is not the text which writes itself.</i> There is a categorical distinction between a thing which evolves according to GPT’s law and the law itself.</p><p id="block221"><span></span><span class="blockquote_pmQ4hgpShcdMkbGZa_1">If we are accustomed to thinking of AI systems as corresponding to agents, it is natural to interpret behavior produced by GPT – say, answering questions on a benchmark test, or writing a blog post – as if it were a human that produced it. We say “GPT answered the question {correctly|incorrectly}” or “GPT wrote a blog post claiming X”, and in doing so attribute the beliefs, knowledge, and intentions revealed by those actions to the actor, GPT (</span><a class="LinkStyles-link" href="/posts/H9knnv8BWGKj6dZim/usd1000-bounty-for-openai-to-show-whether-gpt3-was"><span class="blockquote_pmQ4hgpShcdMkbGZa_1">unless it has ‘deceived’ us</span></a><span class="blockquote_pmQ4hgpShcdMkbGZa_1">).</span></p><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><p id="block222"><span class="blockquote_pmQ4hgpShcdMkbGZa_1">But when grading tests in the real world, we do not say “the laws of physics got this problem wrong” and conclude that the laws of physics haven’t sufficiently mastered the course material. If someone argued this is a reasonable view since the test-taker was steered by none other than the laws of physics, we could point to a different test where the problem was answered correctly by the same laws of physics propagating a different configuration. The “knowledge of course material” implied by test performance is a property of </span><i><span class="blockquote_pmQ4hgpShcdMkbGZa_1">configurations</span></i><span class="blockquote_pmQ4hgpShcdMkbGZa_1">, not physics.</span></p><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><p id="block223">The verdict that knowledge is purely a property of configurations cannot be naively generalized from real life to GPT simulations, because “physics” and “configurations” play different roles in the two (as I’ll address in the next post). The parable of the two tests, however, literally pertains to GPT. People have a tendency to draw <span><span><a href="https://en.wikipedia.org/wiki/Fallacy_of_composition">erroneous global conclusions</a></span></span> about GPT from behaviors which are in fact prompt-contingent, and consequently there is a pattern of constant discoveries that GPT-3 exceeds previously measured capabilities given alternate conditions of generation<span role="doc-noteref" id="fnrefomelvf6lrng" class="footnote-reference"><sup><span><a href="#fnomelvf6lrng" class="">[29]</a></span></sup></span>, which shows no signs of slowing 2 years after GPT-3’s release.</p><p id="block224"><span class="blockquote_v2zcBsGJq7FEXE37L_1">Making the ontological distinction between GPT and instances of text which are propagated by it makes these discoveries unsurprising: obviously, different configurations will be differently capable and in general behave differently when animated by the laws of GPT physics. We can only test one configuration at once, and given the vast number of possible configurations that would attempt any given task, it’s unlikely we’ve found the optimal taker for </span><i><span class="blockquote_v2zcBsGJq7FEXE37L_1">any</span></i><span class="blockquote_v2zcBsGJq7FEXE37L_1"> test.</span></p><span class="blockquote_v2zcBsGJq7FEXE37L_1"></span><p id="block225"><span class="blockquote_pmQ4hgpShcdMkbGZa_1">In the simulation ontology, I say that GPT and its output-instances correspond respectively to the </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">simulator</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"> and </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">simulacra</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">. </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">GPT</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"> is to a </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">piece of text output by GPT</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"> as </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">quantum physics</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"> is to a </span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">person taking a test</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">, or as </span><span><span><a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life#Rules"><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">transition rules of Conway’s Game of Life</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span></a></span></span><span class="blockquote_pmQ4hgpShcdMkbGZa_1"> are to </span><span><span><a href="https://conwaylife.com/wiki/Glider"><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1">glider</span></strong><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span></a></span></span><span class="blockquote_pmQ4hgpShcdMkbGZa_1">. The simulator is a time-invariant law which unconditionally governs the evolution of all simulacra.</span></p><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><p id="block226"><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1674094431/mirroredImages/vJFdjigzmcXMhNTsx/cvecikybt43jhfzufhye.png"/><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span></p><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><p id="block227"><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><i>A meme demonstrating correct technical usage of “simulacra”</i></p><h3 id="Disambiguating_rules_and_automata">Disambiguating rules and automata</h3><p id="block228">Recall the fluid, schizophrenic way that agency arises in GPT’s behavior, so incoherent when viewed through the orthodox agent frame:</p><blockquote id="block229"><p id="block230">In the agentic AI ontology, there is no difference between the policy and the effective agent, but for GPT, there is.</p></blockquote><p id="block231">It’s much less awkward to think of agency as a property of <i>simulacra,</i> as David Chalmers suggests, rather than of the simulator (the policy). Autonomous text-processes propagated by GPT, like automata which evolve according to physics in the real world, have diverse values, simultaneously evolve alongside other agents and non-agentic environments, and are sometimes terminated by the disinterested “physics” which governs them.</p><p id="block232">Distinguishing simulator from simulacra helps deconfuse some frequently-asked questions about GPT which seem to be ambiguous or to have multiple answers, simply by allowing us to specify whether the question pertains to simulator or simulacra. “Is GPT an agent?” is one such question. Here are some others (some frequently asked), whose disambiguation and resolution I will leave as an exercise to readers for the time being:</p><ul><li id="block233">Is GPT <span class=""><a class="LinkStyles-link" href="/w/myopia">myopic</a></span>?</li><li id="block234">Is GPT <span class=""><a class="LinkStyles-link" href="/w/corrigibility">corrigible</a></span>?</li><li id="block235">Is GPT <span><span><a href="https://arxiv.org/abs/2110.10819">delusional</a></span></span>?</li><li id="block236">Is GPT <a class="LinkStyles-link" href="/posts/H9knnv8BWGKj6dZim/usd1000-bounty-for-openai-to-show-whether-gpt3-was">pretending to be stupider than it is</a>?</li><li id="block237">Is GPT computationally equivalent to a <span><span><a href="https://en.wikipedia.org/wiki/Finite-state_machine">finite automaton</a></span></span>?</li><li id="block238">Does GPT <a class="LinkStyles-link" href="/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control">search</a>?</li><li id="block239">Can GPT distinguish correlation and <a class="LinkStyles-link" href="/posts/yZb5eFvDoaqB337X5/investigating-causal-understanding-in-llms">causality</a>?</li><li id="block240">Does GPT have superhuman knowledge?</li><li id="block241">Can GPT <span class=""><a class="LinkStyles-link" href="/w/recursive-self-improvement">write its successor</a></span>?</li></ul><p id="block242">I think that implicit type-confusion is common in discourse about GPT. “GPT”, the neural network, the policy that was optimized, is the easier object to point to and say definite things about. But when we talk about “GPT’s” capabilities, impacts, or alignment, we’re usually actually concerned about the behaviors of an algorithm which calls GPT in an autoregressive loop repeatedly writing to some prompt-state – that is, we’re concerned with simulacra. What we call GPT’s “downstream behavior” is the behavior of simulacra; it is primarily through simulacra that GPT has potential to perform meaningful work (for good or for ill).</p><p id="block243">Calling GPT a simulator gets across that in order to <i>do</i> anything, it has to simulate <i>something</i>, necessarily contingent, and that the thing to do with GPT is to simulate! Most published research about large language models has focused on single-step or few-step inference on closed-ended tasks, rather than <i>processes</i> which evolve through time, which is understandable as it’s harder to get quantitative results in the latter mode. But I think GPT’s ability to simulate text automata is the source of its most surprising and pivotal implications for paths to superintelligence: for how AI capabilities are likely to unfold and for the design-space we can conceive.</p><h2 id="The_limit_of_learned_simulation">The limit of learned simulation</h2><blockquote id="block244"><p id="block245">By 2021, it was blatantly obvious that AGI was imminent. The elements of general intelligence were already known: access to information about the world, the process of predicting part of the data from the rest and then updating one’s model to bring it closer to the truth (…) and the fact that predictive models can be converted into generative models by reversing them: running a prediction model forwards predicts levels of X in a given scenario, but running it backwards predicts which scenarios have a given level of X. A sufficiently powerful system with relevant data, updating to improve prediction accuracy and the ability to be reversed to generate optimization of any parameter in the system is a system that can learn and operate strategically in any domain.</p><p id="block246">– Aiyen’s <a class="LinkStyles-link" href="/posts/YRtzpJHhoFWxbjCso/what-would-it-look-like-if-it-looked-like-agi-was-very-near?commentId=5BGTbapdmtSGajtez">comment</a> on <a class="LinkStyles-link" href="/posts/YRtzpJHhoFWxbjCso/what-would-it-look-like-if-it-looked-like-agi-was-very-near">What would it look like if it looked like AGI was very near?</a></p></blockquote><p id="block247">I knew, before, that the limit of simulation was possible. Inevitable, even, in timelines where exploratory intelligence continues to expand. My own mind attested to this. I took seriously the possibility that my reality could be simulated, and so on.</p><p id="block248">But I implicitly assumed that <span class=""><a class="LinkStyles-link" href="/w/rich_domain">rich domain</a></span> simulations (e.g. simulations containing intelligent sims) would come <i>after</i> artificial superintelligence, not on the way, short of brain uploading. This intuition seems common: in futurist philosophy and literature that I’ve read, pre-SI simulation appears most often in the context of whole-brain emulations.</p><p id="block249">Now I have updated to think that we will live, however briefly, alongside AI that is not yet foom’d but which has <i>inductively</i> learned a rich enough model of the world that it can simulate time evolution of open-ended rich states, e.g. coherently propagate human behavior embedded in the <span class=""><a class="LinkStyles-link" href="/w/real_is_rich">real world</a></span>.</p><p id="block250">GPT updated me on how simulation can be implemented with prosaic machine learning:</p><ul><li id="block251"><strong>Self-supervised ML can create “behavioral” simulations of impressive semantic fidelity.</strong> Whole brain emulation is not necessary to construct convincing and useful virtual humans; it is conceivable that observations of human behavioral traces (e.g. text) are sufficient to reconstruct functionally human-level virtual intelligence.</li><li id="block252"><strong>Learned simulations can be partially observed and lazily-rendered, and still work.</strong> A couple of pages of text severely underdetermines the real-world process that generated text, so GPT simulations are likewise underdetermined. A “partially observed” simulation is more efficient to compute because the state can be much smaller, but can still have the effect of high fidelity as details can be rendered as needed. <span class="blockquote_pmQ4hgpShcdMkbGZa_1">The tradeoff is that it requires the simulator to model semantics – human imagination does this, for instance – which turns out not to be an issue for big models.</span></li><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><li id="block253"><span class="blockquote_pmQ4hgpShcdMkbGZa_1"></span><strong>Learned simulation generalizes impressively.</strong> As I described in the section on behavior cloning, training a model to predict diverse trajectories seems to make it internalize general laws underlying the distribution, allowing it to simulate counterfactuals that can be constructed from the distributional semantics.</li></ul><p id="block254">In my model, these updates dramatically alter the landscape of potential futures, and thus motivate <span><span><a href="https://intelligence.org/files/ExploratoryEngineeringAI.pdf">exploratory engineering</a></span></span> of the class of learned simulators for which GPT-3 is a lower bound. That is the intention of this sequence.</p><h1 id="Next_steps">Next steps</h1><p id="block255">The next couple of posts (if I finish them before the end of the world) will present abstractions and frames for conceptualizing the odd kind of simulation language models do: inductively learned, partially observed / undetermined / lazily rendered, language-conditioned, etc. After that, I’ll shift to writing more specifically about the implications and questions posed by simulators for the alignment problem. I’ll list a few important general categories here:</p><ul><li id="block256"><strong>Novel methods of process/agent specification.</strong> <span class="blockquote_pmQ4hgpShcdMkbGZa_1 blockquote_P9jEh7o6CreEwZZJW_1">Simulators like GPT give us methods of instantiating intelligent processes, including goal-directed agents, with methods other than optimizing against a reward function.</span><ul><span class="blockquote_pmQ4hgpShcdMkbGZa_1 blockquote_P9jEh7o6CreEwZZJW_1"></span><li id="block257"><span class="blockquote_pmQ4hgpShcdMkbGZa_1 blockquote_P9jEh7o6CreEwZZJW_1"></span><strong>Conditioning.</strong> GPT can be controlled to an impressive extent by prompt programming. Conditioning preserves distributional properties in potentially desirable but also potentially undesirable ways, and it’s not clear how out-of-distribution conditions will be interpreted by powerful simulators.<ul><li id="block258">Several posts have been made about this recently:<ul><li id="block259"><a class="LinkStyles-link" href="/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models">Conditioning Generative Models</a>.) and <a class="LinkStyles-link" href="/posts/adiszfnFgPEnRsGSr/conditioning-generative-models-with-restrictions">Conditioning Generative Models with Restrictions</a> by Adam Jermyn</li><li id="block260"><a class="LinkStyles-link" href="/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment">Conditioning Generative Models for Alignment</a> by Jozdien</li><li id="block261"><a class="LinkStyles-link" href="/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models">Training goals for large language models</a> by Johannes Treutlein</li><li id="block262"><a class="LinkStyles-link" href="/posts/HAz7apopTzozrqW2k/strategy-for-conditioning-generative-models">Strategy For Conditioning Generative Models</a> by James Lucassen and Evan Hubinger</li></ul></li><li id="block263">Instead of conditioning on a prompt (&quot;observable&quot; variables), we might also control generative models by <span><span><a href="https://rome.baulab.info/">conditioning on latents</a></span></span>.</li></ul></li><li id="block264"><strong>Distribution specification.</strong> What kind of conditional distributions could be used for training data for a simulator? For example, the <span><span><a href="https://arxiv.org/abs/2106.01345">decision transformer</a></span></span> dataset is constructed for the intent of outcome-conditioning.</li><li id="block265"><strong>Other methods.</strong> When pretrained simulators are modified by methods like <span><span><a href="https://arxiv.org/abs/2009.01325">reinforcement learning from human feedback</a></span></span>, <a class="LinkStyles-link" href="/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project">rejection sampling</a>, <span><span><a href="https://arxiv.org/abs/2203.14465">STaR</a></span></span>, etc, how do we expect their behavior to diverge from the simulation objective?</li></ul></li><li id="block266"><strong>Simulacra alignment.</strong> What can and what should we simulate, and how do we specify/control it?</li><li id="block267"><strong>How does predictive learning generalize?</strong> Many of the above considerations are influenced by how predictive learning generalizes out-of-distribution..<ul><li id="block268">What are the relevant inductive biases?</li><li id="block269">What factors influence generalization behavior?</li><li id="block270">Will powerful models predict <a class="LinkStyles-link" href="/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment">self-fulfilling</a> <a class="LinkStyles-link" href="/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models">prophecies</a>?</li></ul></li><li id="block271"><strong>Simulator inner alignment.</strong> If simulators are not inner aligned, then many important properties like prediction orthogonality may not hold.<ul><li id="block272">Should we expect self-supervised predictive models to be aligned to the simulation objective, or to “care” about some other mesaobjective?</li><li id="block273"><span class="blockquote_P9jEh7o6CreEwZZJW_1 blockquote_P9jEh7o6CreEwZZJW_1">Why mechanistically should mesaoptimizers form in predictive learning, versus for instance in reinforcement learning or GANs?</span></li><span class="blockquote_P9jEh7o6CreEwZZJW_1 blockquote_P9jEh7o6CreEwZZJW_1"></span><li id="block274">How would we test if simulators are inner aligned?</li></ul></li></ul><h1 id="Appendix__Quasi_simulators">Appendix: Quasi-simulators</h1><h2 id="A_note_on_GANs">A note on GANs</h2><p id="block275">GANs and predictive learning with log-loss are both shaped by a causal chain that flows from a single source of information: a ground truth distribution. In both cases the training process is supposed to make the generator model end up producing samples indistinguishable from the training distribution. But whereas log-loss minimizes the generator’s prediction loss against ground truth samples directly, in a GAN setup the generator never directly “sees” ground truth samples. It instead learns through interaction with an intermediary, the discriminator, which does get to see the ground truth, which it references to learn to tell real samples from forged ones produced by the generator. The generator is optimized to produce samples that fool the discriminator.</p><p id="block276">GANs are a form of self-supervised/unsupervised learning that resembles reinforcement learning in methodology. Note that the simulation objective – minimizing prediction loss on the training data – isn’t explicitly represented anywhere in the optimization process. The training losses of the generator and discriminator don’t tell you directly how well the generator models the training distribution, only which model has a relative advantage over the other.</p><p id="block277">If everything goes smoothly, then under unbounded optimization, a GAN setup should create a discriminator as good as possible at telling reals from fakes, which means the generator optimized to fool it should converge to generating samples statistically indistinguishable from training samples. But in practice, inductive biases and failure modes of GANs look very different from those of predictive learning.</p><p id="block278">For example, there’s an <span><span><a href="https://www.gwern.net/Crops#hands">anime GAN</a></span></span> that always draws characters in poses that hide the hands. Why? Because hands are notoriously hard to draw for AIs. If the generator is not good at drawing hands that the discriminator cannot tell are AI-generated, its best strategy locally is to just avoid being in a situation where it has to draw hands (while making it seem natural that hands don’t appear). It can do this, because like an RL policy, it controls the distribution that is sampled, and only samples (and <i>not the distribution</i>) are directly judged by the discriminator.</p><p id="block279">Although GANs arguably share the (weak) simulation objective of predictive learning, their difference in implementation becomes alignment-relevant as models become sufficiently powerful that “failure modes” look increasingly like intelligent deception. We’d expect a simulation by a GAN generator to <span><span><a href="https://developers.google.com/machine-learning/gan/problems#mode-collapse">systematically avoid tricky-to-generate situations</a></span></span> – or, to put it more ominously, systematically try to conceal that it’s a simulator. For instance, a text GAN might subtly steer conversations away from topics which are likely to expose that it isn’t a real human. <i>This</i> is how you get something I’d be willing to call an agent who wants to roleplay accurately.</p><h2 id="Table_of_quasi_simulators">Table of quasi-simulators</h2><p id="block280">Are masked language models simulators? How about non-ML “simulators” like <span><span><a href="https://en.wikipedia.org/wiki/SimCity">SimCity</a></span></span>?</p><p id="block281">In my mind, “simulator”, like most natural language categories, has fuzzy boundaries. <span class="blockquote_AgpYd2hsDNT5TqSRo_1">Below is a table which compares various simulator-like things to the type of simulator that GPT exemplifies on some quantifiable dimensions. The following properties all characterize GPT:</span></p><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><ul><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><li id="block282"><span class="blockquote_AgpYd2hsDNT5TqSRo_1"></span><strong>Self-supervised:</strong> Training samples are self-supervised</li><li id="block283"><strong>Converges to simulation objective:</strong> The system is incentivized to model the transition probabilities of its training distribution faithfully</li><li id="block284"><strong>Generates rollouts:</strong> The model naturally generates rollouts, i.e. serves as a time evolution operator</li><li id="block285"><strong>Simulator / simulacra nonidentity:</strong> There is not a 1:1 correspondence between the simulator and the things that it simulates</li><li id="block286"><strong>Stochastic:</strong> The model outputs probabilities, and so simulates stochastic dynamics when used to evolve rollouts</li><li id="block287"><strong>Evidential:</strong> The input is interpreted by the simulator as partial evidence that informs an uncertain prediction, rather than propagated according to mechanistic rules</li></ul><figure class="table"><table><tbody><tr><td> </td><td><strong>Self-supervised</strong></td><td><strong>Converges to simulation objective</strong></td><td><strong>Generates rollouts</strong></td><td><strong>Simulator / simulacra nonidentity</strong></td><td><strong>Stochastic</strong></td><td><strong>Evidential</strong></td></tr><tr><td><strong>GPT</strong></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>Bert</strong></td><td>X</td><td>X</td><td> </td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>“Behavior cloning”</strong></td><td>X</td><td>X</td><td>X</td><td> </td><td>X</td><td>X</td></tr><tr><td><strong>GANs</strong></td><td>X<span role="doc-noteref" id="fnrefbfhs37ysptj" class="footnote-reference"><sup><span><a href="#fnbfhs37ysptj" class="">[30]</a></span></sup></span></td><td>?</td><td> </td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>Diffusion</strong></td><td>X<span role="doc-noteref" id="fnrefbfhs37ysptj" class="footnote-reference"><sup><span><a href="#fnbfhs37ysptj" class="">[30]</a></span></sup></span></td><td>?</td><td> </td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>Model-based RL transition function</strong></td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>Game of life</strong></td><td> </td><td>N/A</td><td>X</td><td>X</td><td> </td><td> </td></tr><tr><td><strong>Physics</strong></td><td> </td><td>N/A</td><td>X</td><td>X</td><td>X</td><td> </td></tr><tr><td><strong>Human imagination</strong></td><td>X<span role="doc-noteref" id="fnrefyqbdfj6rki8" class="footnote-reference"><sup><span><a href="#fnyqbdfj6rki8" class="">[31]</a></span></sup></span></td><td> </td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td><strong>SimCity</strong></td><td> </td><td>N/A</td><td>X</td><td>X</td><td>X</td><td> </td></tr></tbody></table></figure><ol role="doc-endnotes" class="footnotes"><li role="doc-endnote" id="fndbsg8o0p25q" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefdbsg8o0p25q">^</a></strong></sup></span><div class="footnote-content"><p id="block289"><span><span><a href="https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf">Prediction and Entropy of Printed English</a></span></span></p></div></li><li role="doc-endnote" id="fna35qx2ldayo" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefa35qx2ldayo">^</a></strong></sup></span><div class="footnote-content"><p id="block291">A few months ago, I asked Karpathy whether he ever thought about what would happen if language modeling actually worked someday when he was implementing char-rnn and writing <span><span><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><u>The Unreasonable Effectiveness of Recurrent Neural Networks</u></a></span></span>. No, he said, and he seemed similarly mystified as myself as to why not.</p></div></li><li role="doc-endnote" id="fnhxtnxj1c2hb" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefhxtnxj1c2hb">^</a></strong></sup></span><div class="footnote-content"><p id="block293">“Unsurprisingly, size matters: when training on a very large and complex data set, fitting the training data with an LSTM is fairly challenging. Thus, the size of the LSTM layer is a very important factor that influences the results(...). The best models are the largest we were able to fit into a GPU memory.”</p></div></li><li role="doc-endnote" id="fnx2xlz0klyh8" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefx2xlz0klyh8">^</a></strong></sup></span><div class="footnote-content"><p id="block295">It strikes me that this description may evoke “oracle”, but I’ll argue shortly that this is not the limit which prior usage of “oracle AI” has pointed to.</p></div></li><li role="doc-endnote" id="fn3ifsldmwtxr" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnref3ifsldmwtxr">^</a></strong></sup></span><div class="footnote-content"><p id="block297"><span><span><a href="https://arxiv.org/abs/2205.15241">Multi-Game Decision Transformers</a></span></span></p></div></li><li role="doc-endnote" id="fnoti8ojhy48a" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefoti8ojhy48a">^</a></strong></sup></span><div class="footnote-content"><p id="block299">from <span><span><a href="https://dailynous.com/2020/07/30/philosophers-gpt-3/#chalmers">Philosophers On GPT-3</a></span></span></p></div></li><li role="doc-endnote" id="fn8p9svyjn8cv" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnref8p9svyjn8cv">^</a></strong></sup></span><div class="footnote-content"><p id="block301">[citation needed]</p></div></li><li role="doc-endnote" id="fn5z0xgsu2zo5" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnref5z0xgsu2zo5">^</a></strong></sup></span><div class="footnote-content"><p id="block303">they are not <a class="LinkStyles-link" href="/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals"><u>wrapper</u></a> <a class="LinkStyles-link" href="/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy"><u>minds</u></a></p></div></li><li role="doc-endnote" id="fnn3u1lofwts9" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefn3u1lofwts9">^</a></strong></sup></span><div class="footnote-content"><p id="block305">although a simulated character might, if they knew what was happening.</p></div></li><li role="doc-endnote" id="fnaxtq4tiuhug" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefaxtq4tiuhug">^</a></strong></sup></span><div class="footnote-content"><p id="block307">You might say that it’s the will of a different agent, the author. But this pattern is learned from accounts of <a class="LinkStyles-link" href="/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god"><u>real life</u></a> as well.</p></div></li><li role="doc-endnote" id="fnzjj9iwtc3or" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefzjj9iwtc3or">^</a></strong></sup></span><div class="footnote-content"><p id="block309">Note that this formulation assumes inner alignment to the prediction objective.</p></div></li><li role="doc-endnote" id="fnrhs2red4hto" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefrhs2red4hto">^</a></strong></sup></span><div class="footnote-content"><p id="block311">Note that this is a distinct claim from that of <span class=""><a class="LinkStyles-link" id="nyEFg3AuJpdAozmoX" href="/s/nyEFg3AuJpdAozmoX"><u>Shard Theory</u></a></span>, which says that the effective agent(s) will not optimize for the outer objective <i>due to inner misalignment. </i>Predictive orthogonality refers to the outer objective and the form of idealized inner-aligned policies.</p></div></li><li role="doc-endnote" id="fn4qv9vfo4ps7" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnref4qv9vfo4ps7">^</a></strong></sup></span><div class="footnote-content"><p id="block313">In the Eleuther discord</p></div></li><li role="doc-endnote" id="fnual5wnttct" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefual5wnttct">^</a></strong></sup></span><div class="footnote-content"><p id="block315">And if there is an inner alignment failure such that GPT forms preferences over the consequences of its actions, it’s not clear a priori that it will care about non-myopic text prediction over something else.</p></div></li><li role="doc-endnote" id="fngmvcllz9xem" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefgmvcllz9xem">^</a></strong></sup></span><div class="footnote-content"><p id="block317">Having spoken to Gwern since, his perspective seems more akin to seeing physics as an agent that <span><span><a href="https://en.wikipedia.org/wiki/Principle_of_minimum_energy"><u>minimizes free energy</u></a></span></span>, a <span><span><a href="https://en.wikipedia.org/wiki/Free_energy_principle"><u>principle</u></a></span></span> which extends into the domain of self-organizing systems. I think this is a nuanced and valuable framing, with a potential implication/hypothesis that dynamical world models like GPT must learn the same type of optimizer-y cognition as agentic AI.</p></div></li><li role="doc-endnote" id="fnsuexqono1oi" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefsuexqono1oi">^</a></strong></sup></span><div class="footnote-content"><p id="block319">except arguably log-loss on a self-supervised test set, which isn’t very interpretable</p></div></li><li role="doc-endnote" id="fnx2cab1frycp" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefx2cab1frycp">^</a></strong></sup></span><div class="footnote-content"><p id="block321">The way GPT is trained actually processes each token as question and answer simultaneously.</p></div></li><li role="doc-endnote" id="fngmdpgm15gb4" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefgmdpgm15gb4">^</a></strong></sup></span><div class="footnote-content"><p id="block323">One could argue that the focus on closed-ended tasks is necessary for benchmarking language models. Yes, and the focus on capabilities measurable with standardized benchmarks is part of the supervised learning mindset. </p></div></li><li role="doc-endnote" id="fnp95narkokz" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefp95narkokz">^</a></strong></sup></span><div class="footnote-content"><p id="block325">to abuse the term</p></div></li><li role="doc-endnote" id="fnj2g0f81c6p" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefj2g0f81c6p">^</a></strong></sup></span><div class="footnote-content"><p id="block327">Every usage of the word “question” here is in the functional, not semantic or grammatical sense – any prompt is a question for GPT.</p></div></li><li role="doc-endnote" id="fnn13gtuadzp" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefn13gtuadzp">^</a></strong></sup></span><div class="footnote-content"><p id="block329">Of course, there are also other interventions we can make except asking the right question at the beginning.</p></div></li><li role="doc-endnote" id="fn6m7m5wc4xjx" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnref6m7m5wc4xjx">^</a></strong></sup></span><div class="footnote-content"><p id="block331">table from <span><span><a href="https://arxiv.org/abs/2203.02155"><u>“Training language models to follow instructions with human feedback”</u></a></span></span></p></div></li><li role="doc-endnote" id="fnbjl6s2y0l5a" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefbjl6s2y0l5a">^</a></strong></sup></span><div class="footnote-content"><p id="block333">Jean Baudrillard, Simulacra and Simulation</p></div></li><li role="doc-endnote" id="fnco7whsfoh2e" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefco7whsfoh2e">^</a></strong></sup></span><div class="footnote-content"><p id="block335">A <span><span><a href="https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules"><u>proper scoring rule</u></a></span></span> is optimized by predicting the “true” probabilities of the distribution which generates observations, and thus incentivizes honest probabilistic guesses. Log-loss (such as GPT is trained with) is a proper scoring rule.</p></div></li><li role="doc-endnote" id="fnfv0wlygqa25" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnreffv0wlygqa25">^</a></strong></sup></span><div class="footnote-content"><p id="block337">Predictive accuracy is deontological with respect to the output as an <i>action</i>, but may still incentivize instrumentally convergent inner implementation, with the output prediction itself as the “consequentialist” objective.</p></div></li><li role="doc-endnote" id="fncrt8wagfir9" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefcrt8wagfir9">^</a></strong></sup></span><div class="footnote-content"><p id="block339">This isn’t strictly true because of attention gradients: GPT&#x27;s computation is optimized not only to predict the next token correctly, but also to <span class=""><span></span>cause </span><i><span class="">future tokens to be predicted correctly</span></i><span class=""> when looked up by attention</span>. I may write a post about this in the future.</p></div></li><li role="doc-endnote" id="fni3y95l8d8bo" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefi3y95l8d8bo">^</a></strong></sup></span><div class="footnote-content"><p id="block341">actually, the <span><span><a href="https://generative.ink/posts/language-models-are-multiverse-generators/">multiverse</a></span></span>, if physics is stochastic</p></div></li><li role="doc-endnote" id="fnmo7z3w9i52s" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefmo7z3w9i52s">^</a></strong></sup></span><div class="footnote-content"><p id="block343">The reason we don’t see a bunch of simulated alternate universes after humans guessed the laws of physics is because our reality has a huge state vector, making evolution according to the laws of physics infeasible to compute. Thanks to locality, we do have simulations of small configurations, though.</p></div></li><li role="doc-endnote" id="fnomelvf6lrng" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefomelvf6lrng">^</a></strong></sup></span><div class="footnote-content"><p id="block345">Prompt programming only: <span><span><a href="https://arxiv.org/abs/2102.07350"><u>beating OpenAI few-shot benchmarks with 0-shot prompts</u></a></span></span>, <span><span><a href="https://generative.ink/posts/list-sorting-does-not-play-well-with-few-shot/"><u>400% increase in list sorting accuracy with 0-shot Python prompt</u></a></span></span>, <span><span><a href="https://arxiv.org/abs/2102.09690"><u>up to 30% increase in benchmark accuracy from changing the order of few-shot examples</u></a></span></span>, and, uh, <span><span><a href="https://twitter.com/BlancheMinerva/status/1537952688972787713"><u>30% increase in accuracy after capitalizing the ground truth</u></a></span></span>. And of course, factored cognition/chain of thought/inner monologue: check out this awesome <span><span><a href="https://www.gwern.net/docs/ai/nn/transformer/gpt/inner-monologue/"><u>compilation</u></a></span></span> by Gwern.</p></div></li><li role="doc-endnote" id="fnbfhs37ysptj" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefbfhs37ysptj">^</a></strong></sup></span><div class="footnote-content"><p id="block347">GANs and diffusion models can be unconditioned (unsupervised) or conditioned (self-supervised) </p></div></li><li role="doc-endnote" id="fnyqbdfj6rki8" class="footnote-item"><span class="footnote-back-link"><sup><strong><a href="#fnrefyqbdfj6rki8">^</a></strong></sup></span><div class="footnote-content"><p id="block349">The human imagination is surely shaped by self-supervised learning (predictive learning on e.g. sensory datastreams), but probably also other influences, including innate structure and reinforcement. </p></div></li></ol></div></div></div></div></div></div></div><div class="MultiToCLayout-rhs" style="grid-area:rhs0"><div class="PostsPage-reserveSpaceForSidenotes"></div><div class="SideItems-sidebar"></div></div><div class="MultiToCLayout-gap1"></div><div class="MultiToCLayout-content" style="grid-area:content1"><div class="PostsPage-centralColumn PostsPage-betweenPostAndComments"><!--$?--><template id="B:7"></template><!--/$--></div></div><div style="grid-area:toc2" class="MultiToCLayout-toc MultiToCLayout-commentToCMargin MultiToCLayout-splashPageHeaderToc"><div class="MultiToCLayout-stickyBlockScroller MultiToCLayoutStickyBlockScroller MultiToCLayout-commentToCIntersection"><div class="MultiToCLayout-stickyBlock"><div class="CommentsTableOfContents-root"><a id="comments-table-of-contents" href="#" class="CommentsTableOfContents-postTitle">Simulators</a><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#oqK8ELfJGxvKRLs5N" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">56</span><span class="UsersNameDisplay-noColor">Charlie Steiner</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#sCgpuvjawdpaujqSm" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">14</span><span class="UsersNameDisplay-noColor">Capybasilisk</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#5HerQdag98EEr6Gwa" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#hAm9dnw3jwjWsZtcE" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Charlie Steiner</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#or9QbHaPmLsiYgsSQ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#Xs9PkNbCCkW8kJYa6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Charlie Steiner</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#sTcnNETxKZGdzfMyZ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Vladimir_Nesov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#FZDseetazfLBouh7q" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">47</span><span class="UsersNameDisplay-noColor">Bird Concept</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#dFsJBsYLoWC4GKbaH" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#fFAmcr8qdR34bDDfi" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">38</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#3B95MjRv4gzmtNtth" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">17</span><span class="UsersNameDisplay-noColor">Zack_M_Davis</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#AgpYd2hsDNT5TqSRo" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">15</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#tDXfHQbrksfKKibKy" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">TurnTrout</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#L4hoRxcE4RHjTxmHD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#EKBkwTe4YgatyA673" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#Amd5cTjNCcLGWcyaM" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#kwhqir7W9BavDGB5X" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">Charlie Steiner</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#dkJf3WmhEqsihBQFw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#8TzNEtmXmms9y37ab" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Charlie Steiner</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#cJnGL6ERDmpxE7KyH" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#gumPfFHAitsQnhaxd" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#xuB5wnYoyTrEQerdW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#AovHhK6FnELkFrvGt" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#Jh52tgFngK3sz2eJd" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#ryFe3MQ3EejsBqPvm" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#Mh78GiZjLGpdaWHSL" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#Jor3ZuzwDNjCMd9Sz" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#bS2TAhs7Npn3fM6tK" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#qqZKcRgWX7Cwujym6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#eA49HjAC5oJWxw5bw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#HSgSCdsNyuBTPm2Fb" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#jBgHEjN7TYdjMYeom" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#bpgdEGESQhDowdvGa" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#p7t42ziTsiyxg8M7T" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Rohin Shah</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#ioZh6gcxEmCQwapCi" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#3aTEvecR7o2Re4SAw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Rohin Shah</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#iZC8BBj5Cap9BCdsJ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">habryka</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#Nvm2emKC5KdCbcG5j" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Rohin Shah</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#f3qFxFcBaEwwKqnKD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#waADBWJyFhnE897x4" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Fiora Starlight</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#gavPZ7p47htKr4PnQ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">32</span><span class="UsersNameDisplay-noColor">Capybasilisk</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#EouDrGb8YJE4jxrmF" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">30</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#dG3vEYLrfmNN85yzw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">27</span><span class="UsersNameDisplay-noColor">Polite Infinity</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#qQYwZ6Eqm2ztGzRpf" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">15</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#QoAwdue9gHYqw3nMd" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Mitchell_Porter</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#99cXcPNsNTxpioso6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">tailcalled</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#gPz3iJkKjLmxXnDok" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">eggsyntax</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#SCZYQgi2gzCkSke7w" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">bridgebot</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#d5K6nxMfcDAvs2eTh" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">kromem</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#umFefRzknEHTfdopL" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">27</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#dF4ZW3f2jx5d7ceHj" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">24</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#zqAYwqSrixiYkhE8n" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#RqFJQLGipbzxMWz4h" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#vQyJXdcZxGaJAuMCL" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">23</span><span class="UsersNameDisplay-noColor">nostalgebraist</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#G8zNtoHBqCoisXzcy" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">23</span><span class="UsersNameDisplay-noColor">Scott Emmons</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#sjvJxEzkyq89hsjEA" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">10</span><span class="UsersNameDisplay-noColor">Scott Emmons</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#hTsQG5RjDiwfJcvux" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#kdcxEDesSsqDLDjzf" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">20</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#weBdayHJw7rryQMuP" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">18</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#EyadzTeBBiJ9D8jCe" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Domenic</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#od6WQBbaxh2Q3eYG7" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#EFrdd6iJqTfNet9nu" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#uEZfarrN2A5SHGkbX" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#9XuHTAtotCcax5vpG" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#HEgKtuqwP8m9aRvbN" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">22</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#SdLkarjhWLZz3xTJr" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Vladimir_Nesov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#wTFzPzXLGzamEsJXc" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">metasemi</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#aaqwnHBCq6dx3JyZv" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#7niR2Fvx3FhdeGpe5" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">8</span><span class="UsersNameDisplay-noColor">JenniferRM</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#ixy8dFdBrRg3GeKDx" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">MSRayne</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#g3gYRTKbmCm3JhwmW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#zHMjbdzXdikiBbGHq" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#JfaayD5sqLBQrDsCn" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Roman Leventov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#KbhyCWNSGPj3oJz5g" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">19</span><span class="UsersNameDisplay-noColor">David Scott Krueger (formerly: capybaralet)</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#BPBLxvC9vzxvpcsTy" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">15</span><span class="UsersNameDisplay-noColor">Alex Lawsen</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#2a62QzCrkGnK2NSGw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">18</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#v3W6HXA6NsswavZA4" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">elifland</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#PMXaBriL3z8Y7jdfG" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Alex Lawsen</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#YeYFgiPC5grRZXc9n" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">elifland</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#J6nccirDiK4gkTZa9" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#ZEN2yj5gSw7duroKH" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">Joe Collman</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#iYwjFxh8NkfoiFu7E" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">21</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#qJfYCbhWrzksSXz2x" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Noosphere89</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#ucpw4nXpyM9oq25p3" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Seth Herd</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#kcY7SWu6bvHBYgJ9P" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">elifland</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#wEHBFj9t6afYifheR" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">14</span><span class="UsersNameDisplay-noColor">Garrett Baker</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#gBX9f7dWYpcdHhNwm" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">14</span><span class="UsersNameDisplay-noColor">Nathan Helm-Burger</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#8mfdpNTqDKddi4Chg" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">Adam Jermyn</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#Gnd3pEsphdJivSwdJ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#2qbuujMBJcEwo7ngk" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Adam Jermyn</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#L7y3Cfjqxr3rgJKiS" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#pmQ4hgpShcdMkbGZa" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">Roman Leventov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#jCJgRuHegNBesRqQC" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">Sodium</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#DKe3GX3c4KeYGtsmJ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">TurnTrout</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#Gch2j6EudsWdndLkS" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">Solenoid_Entity</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#t5Gxfpf72b9coAo2n" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">15</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#ewMwc4ch2br2pHXwk" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Not Relevant</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#RqALs3cssRqLNbXAD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">10</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#u9utnnKv5d2DGdtMz" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">MikkW</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#7shzcqputFSqANDXu" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">Vladimir_Nesov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#kPQFvdHrHzbkgwjm6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">8</span><span class="UsersNameDisplay-noColor">metachirality</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#v2zcBsGJq7FEXE37L" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">8</span><span class="UsersNameDisplay-noColor">David Udell</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#mbjRCkWEWXCa9ehJD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">12</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#ygKfqHfZAuhXXuzsL" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">MiguelDev</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#E3o7JepvJSKr5StWC" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#mxrySWWWdoncEPmBk" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#5NRydRaaw3ttn7hRk" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">9</span><span class="UsersNameDisplay-noColor">Ramana Kumar</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#6kfEvR7ttMnEx98rS" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#rcGuh4hQguXkXaa3M" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">Logan Riggs</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#XwptucdtJJbJrjhvc" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#5Nt6xtmhzQCWzHf6d" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#n7k9bH4HH8txhXQeE" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#tRccZ3ymNHumHxkyd" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Logan Riggs</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#fLB5RGbaFhCSWegio" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#LcWqmFWYaTokFJYoq" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Jay Bailey</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#D6hrQ2cFiqnLZRxGA" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Logan Riggs</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#CCiT53zcyHYtzpxRb" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span>[comment deleted]</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#LcuZNGTZu3wCuPjCv" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#rnfBME5fgmEHnjhiy" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#hAwH35iKn3uwcMqvX" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Benjy Forstadt</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#SNnpCLxCSBNgTy88e" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#qYggjjAKber46A7iG" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">0</span><span>[comment deleted]</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#AdftaWh3TkHLa9DKM" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Dan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#vLiEkpo3Da6BwrRTX" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Vika</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#fpx4XK5JAHM44qh6w" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">VojtaKovarik</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#BfsbG2guKQPw7Fx4i" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Vika</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#KGMvxTrHNeLseHpT6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">the gears to ascension</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#gmxc7TwaWNJuFw4wT" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Jay Bailey</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#WRsxA8gagGTaDBdeP" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">Razied</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#HthBxwD7Hvd45Yne9" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Jay Bailey</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#w8FWcqh6QbYqtwZsv" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">13</span><span class="UsersNameDisplay-noColor">Razied</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#NsdpkcyrEfwno6Rhc" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span>[comment deleted]</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#gDamrghDuZEZuMWG6" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">Prometheus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#P9jEh7o6CreEwZZJW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">TurnTrout</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#8N7anBfTK7otmnfSG" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">TurnTrout</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#CPTkunPPEaFhjFBbJ" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">10</span><span class="UsersNameDisplay-noColor">ryan_greenblatt</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#LY7qqdkaM48EeAAFw" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">TurnTrout</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#qSB93LjZNm8pAAf5c" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">quetzal_rainbow</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#k4gqKQYNqX3yAtpyu" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">the gears to ascension</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#KWJtt5FH3KhwSDsHf" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Writer</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#LrGngJBuyyuNwAzLu" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">quetzal_rainbow</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#qZokadzqsLFBPFwGf" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">Bogdan Ionut Cirstea</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#Gso6FCBLFFhYiEBRt" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">catubc</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#RojbaTNwg83n8AFAy" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">11</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#FCvc3tMvLQwMLkm5i" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">catubc</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#BdEZ7GzLb9LgkEsRz" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">cousin_it</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#gkSA8mLGdsyPsYhZW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">Sam Ringer</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#9mdrDYmhvNgyQbNK7" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">NicholasKees</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#R839eeM9hTBtMSHjF" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">esthle Amitace</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#ua6xGqoiJkCuqtdfE" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">MiguelDev</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#8bedc3uoz7954fxBM" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#rvdJqFuHhKTEYq2kD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">SydneyFan</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#KCbdabcjP6oBru2HW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Jan_Kulveit</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#4gMF3CCPmzi4bx7ed" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Past Account</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#Z9oajGshrZtCYBS8W" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">7</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#TojpsCqJhZYB6apSk" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">Past Account</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#cZ2K78ehCJgLkiqku" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">5</span><span class="UsersNameDisplay-noColor">VojtaKovarik</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#JvRiRAZY34vJ9GgPD" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">delton137</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#Txix2wun7zGCeFcyW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">6</span><span class="UsersNameDisplay-noColor">the gears to ascension</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#LGBysJtEshk35WuBW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">4</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#rv9HqxBYsWCkKuRxd" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">RogerDearnaley</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#pKpfegbGbxYsCvRtg" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">Fergus Fettes</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#u93NFn8vxapFqtRjH" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">MiguelDev</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#DuYCLcbi8tE3DgQnX" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">aviv</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#tBrsot2oYTqbpRrgx" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">1</span><span class="UsersNameDisplay-noColor">domenicrosati</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level1"><a href="#PpymDqWWxttapcqXA" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">0</span><span class="UsersNameDisplay-noColor">Gunnar_Zarncke</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#LRZwCZYatw3ww93N5" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">23</span><span class="UsersNameDisplay-noColor">Vladimir_Nesov</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#mbjATDfHg8pwz4LiM" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">Gunnar_Zarncke</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#WnACiiXGEbXy5c93t" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#WhL6KpZCtWsWANHZe" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Gunnar_Zarncke</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level2"><a href="#xmyZDypgNpEzxJ7rG" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level3"><a href="#drqWk2nMEyyhMt5gM" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">2</span><span class="UsersNameDisplay-noColor">Gunnar_Zarncke</span></span></a></div><div class="TableOfContentsRow-root TableOfContentsRow-level4"><a href="#NABdhHCpP2FwDvQpW" class="TableOfContentsRow-link TableOfContentsRow-highlightDot TableOfContentsRow-dense"><span class="CommentsTableOfContents-comment"><span class="CommentsTableOfContents-commentKarma">3</span><span class="UsersNameDisplay-noColor">janus</span></span></a></div></div></div></div></div><div class="MultiToCLayout-gap1"></div><div class="MultiToCLayout-content" style="grid-area:content2"><span><span><div class="PostsPage-commentsSection"><!--$?--><template id="B:8"></template><!--/$--></div></span></span></div><div class="MultiToCLayout-gap1"></div><div class="MultiToCLayout-content" style="grid-area:content3"><!--$?--><template id="B:9"></template><!--/$--></div></div><div class="MultiToCLayout-tocFooter"><div class="LWCommentCount-root"><a class="LWCommentCount-comments LWCommentCount-wideClickTarget" href="#comments"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="LWCommentCount-commentsIcon ForumIcon-root"><path stroke-linecap="round" stroke-linejoin="round" d="M2.25 12.76c0 1.6 1.123 2.994 2.707 3.227 1.087.16 2.185.283 3.293.369V21l4.076-4.076a1.526 1.526 0 011.037-.443 48.282 48.282 0 005.68-.494c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0012 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741v6.018z"></path></svg>170<span class="LWCommentCount-commentsLabel ToCRowHover LWCommentCount-rowOpacity">Comments</span></a></div></div></div></div><div style="height:56px" class="PostsBottomBar-headroom headroom-wrapper"><div class="headroom headroom--unfixed"><div class="PostsBottomBar-root"><span class="LWTooltip-root"><div class="PostsBottomBar-button"><svg class="MuiSvgIcon-root PostsBottomBar-icon PostsBottomBar-backArrow ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 4l-1.41 1.41L16.17 11H4v2h12.17l-5.58 5.59L12 20l8-8z"></path></svg></div></span><span class="LWTooltip-root"><div class="PostsBottomBar-button"><svg class="MuiSvgIcon-root PostsBottomBar-icon PostsBottomBar-upArrow ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 4l-1.41 1.41L16.17 11H4v2h12.17l-5.58 5.59L12 20l8-8z"></path></svg></div></span><span class="LWTooltip-root"><div class="PostsBottomBar-button"><div class="PostsBottomBar-commentsButton"><svg class="MuiSvgIcon-root PostsBottomBar-commentIcon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsBottomBar-commentCount">169</div></div></div></span><div class="PostsBottomBar-actionsButton"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon PostsBottomBar-actionsIcon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div></div></div></div></div><script>$RS=function(a,b){a=document.getElementById(a);b=document.getElementById(b);for(a.parentNode.removeChild(a);a.firstChild;)b.parentNode.insertBefore(a.firstChild,b);b.parentNode.removeChild(b)};$RS("S:5","P:5")</script><script>$RC("B:3","S:3")</script><div hidden id="S:7"><!--$--><div class="PostsPagePostFooter-footerTagList"><span class="FooterTagList-root"><span class=""><span class="FooterTag-root"><a href="/w/simulator-theory"><span class="FooterTag-name">Simulator Theory</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/language-models-llms"><span class="FooterTag-name">Language Models (LLMs)</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/llm-personas"><span class="FooterTag-name">LLM Personas</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/gpt"><span class="FooterTag-name">GPT</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/outer-alignment"><span class="FooterTag-name">Outer Alignment</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/simulation-1"><span class="FooterTag-name">Simulation</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/corrigibility-1"><span class="FooterTag-name">Corrigibility</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/deconfusion"><span class="FooterTag-name">Deconfusion</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/myopia"><span class="FooterTag-name">Myopia</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/oracle-ai"><span class="FooterTag-name">Oracle AI</span></a></span></span><span class=""><span class="FooterTag-root"><a href="/w/tool-ai"><span class="FooterTag-name">Tool AI</span></a></span></span><span class=""><span class="FooterTag-root FooterTag-core"><a href="/w/ai"><span class="FooterTag-name">AI</span></a></span></span><a class="FooterTagList-postTypeLink" href="/recommendations"><span class="LWTooltip-root"><div class="FooterTagList-frontpageOrPersonal">Curated</div></span></a></span></div><!--/$--><div class="PostsPagePostFooter-footerSection"><div class="PostsPagePostFooter-voteBottom PostsPagePostFooter-lwVote"><div class="PostsVoteDefault-voteBlock"><div class="PostsVoteDefault-upvote"><button class="VoteButton-root VoteArrowIconHollow-up" type="button"><span class="VoteButton-inner"><svg class="VoteArrowIconHollow-smallArrow" viewBox="6 6 12 12"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></div><div class="PostsVoteDefault-voteScores"><span><h1 class="Typography-root Typography-headline PostsVoteDefault-voteScore PostsVoteDefault-voteScoreFooter">690</h1></span><span><h1 class="Typography-root Typography-headline PostsVoteDefault-voteScore PostsVoteDefault-secondaryVoteScore PostsVoteDefault-voteScoreFooter">Ω <!-- -->142</h1></span></div><div class="PostsVoteDefault-downvote"><button class="VoteButton-root VoteArrowIconHollow-down" type="button"><span class="VoteButton-inner"><svg class="VoteArrowIconHollow-smallArrow" viewBox="6 6 12 12"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></div></div></div></div><div class="PostsPagePostFooter-bottomNavigation"><div class="BottomNavigation-root"><div class="BottomNavigation-divider"></div><div class="BottomNavigation-post BottomNavigation-nextPost"><span><a href="/s/N7nDePaNabJdnbXeE/p/DSEwkvj8W7y8C3jau"><div class="BottomNavigationItem-root"><div class="BottomNavigationItem-direction">Next<!-- -->:</div><div class="BottomNavigationItem-postTitle">Simulators, constraints, and goal agnosticism: porbynotes vol. 1</div><div class="BottomNavigationItem-meta"><span class="BottomNavigationItem-metaEntry">2<!-- --> comments</span><span class="BottomNavigationItem-metaEntry">40<!-- --> karma</span></div></div></a><span class="BottomNavigationItem-login"><span class="LWTooltip-root"><a class="LoginPopupButton-root">Log in to save where you left off</a></span></span></span></div><div class="BottomNavigation-clear"></div></div></div><!--$?--><template id="B:a"></template><!--/$--></div><script>$RC("B:7","S:7")</script><script>$RC("B:8","S:8")</script><div hidden id="S:9"><div class="PostBottomRecommendations-root"><div><div class="PostBottomRecommendations-section"><div class="PostBottomRecommendations-sectionHeading">More from<!-- --> <span class=""><a href="/users/janus-1">janus</a></span></div><div class="Loading-spinner"><div class="Loading-bounce1"></div><div class="Loading-bounce2"></div><div class="Loading-bounce3"></div></div><div class="PostBottomRecommendations-viewMore"><a href="/users/janus-1">View more</a></div></div><div class="PostBottomRecommendations-section"><div class="PostBottomRecommendations-sectionHeading">Curated and popular this week</div><div class="Loading-spinner"><div class="Loading-bounce1"></div><div class="Loading-bounce2"></div><div class="Loading-bounce3"></div></div></div></div></div></div><script>$RC("B:9","S:9")</script><div hidden id="S:6"><div class="ReviewPillContainer-root"><span class="LWTooltip-root"><a href="#fFAmcr8qdR34bDDfi"><div class="ReviewPillContainer-review">Review by<div class="ReviewPillContainer-reviewerName"><span class="UsersNameDisplay-noColor">habryka</span></div></div></a></span><span class="LWTooltip-root"><a href="#EouDrGb8YJE4jxrmF"><div class="ReviewPillContainer-review">Review by<div class="ReviewPillContainer-reviewerName"><span class="UsersNameDisplay-noColor">janus</span></div></div></a></span><span class="LWTooltip-root"><a href="#vQyJXdcZxGaJAuMCL"><div class="ReviewPillContainer-review">Review by<div class="ReviewPillContainer-reviewerName"><span class="UsersNameDisplay-noColor">nostalgebraist</span></div></div></a></span><span class="LWTooltip-root"><a href="#jCJgRuHegNBesRqQC"><div class="ReviewPillContainer-review">Review by<div class="ReviewPillContainer-reviewerName"><span class="UsersNameDisplay-noColor">Sodium</span></div></div></a></span><span class="LWTooltip-root"><a href="#RqALs3cssRqLNbXAD"><div class="ReviewPillContainer-review">Review by<div class="ReviewPillContainer-reviewerName"><span class="UsersNameDisplay-noColor">RogerDearnaley</span></div></div></a></span></div></div><script>$RC("B:6","S:6")</script><div hidden id="S:a"><div class="PingbacksList-root"><div class="PingbacksList-title"><span class="LWTooltip-root"><span>Mentioned in</span></span></div><div class="PingbacksList-list"><div><div class="Pingback-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">647</span></span><span class=""><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post"><span>The Waluigi Effect (mega-post)</span></a></span></span></span></div></div><div><div class="Pingback-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">366</span></span><span class=""><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/epjuxGnSPof3GnMSL/alignment-remains-a-hard-unsolved-problem"><span>Alignment remains a hard, unsolved problem</span></a></span></span></span></div></div><div><div class="Pingback-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">363</span></span><span class=""><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky"><span>My Objections to &quot;We’re All Gonna Die with Eliezer Yudkowsky&quot;</span></a></span></span></span></div></div><div><div class="Pingback-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">335</span></span><span class=""><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/bxt7uCiHam4QXrQAA/cyborgism"><span>Cyborgism</span></a></span></span></span></div></div><div><div class="Pingback-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">321</span></span><span class=""><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/mzvu8QTRXdvDReCAL/gentleness-and-the-artificial-other"><span>Gentleness and the artificial Other</span></a></span></span></span></div></div></div><a class="LoadMore-root PingbacksList-loadMore" href="#">Load More (5/182)</a></div></div><script>$RC("B:a","S:a")</script></body></html>
